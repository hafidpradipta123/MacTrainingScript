{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Dense layer\n",
    "class Layer_Dense :\n",
    "# Layer initialization\n",
    "    def __init__ ( self , n_inputs , n_neurons , weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, bias_regularizer_l1 = 0, bias_regularizer_l2 =0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros(( 1 , n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        #set regularization strength\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        #Gradients on regularization\n",
    "        #L1 on weights\n",
    "\n",
    "        if self.weight_regularizer_l1 >0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights <0 ] =-1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        #l2 on weights\n",
    "        if self.weight_regularizer_l2 >0:\n",
    "            self.dweights += 2*self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        #L1 regularization - biases\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases< 0 ] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        #l2 on biases\n",
    "        if self.bias_regularizer_l2> 0:\n",
    "            self.dbiases +=  2 * self.bias_regularizer_l2 * self.biases\n",
    "        #Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Layer_Dropout:\n",
    "    #init\n",
    "    def __init__(self, rate):\n",
    "        #store the rate, we intert it as for example for dropout of 0.1, we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #save input values\n",
    "        self.inputs = inputs\n",
    "        #Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size = inputs.shape) / self.rate\n",
    "        #apply mask to output value\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "    # ReLU activation\n",
    "class Activation_ReLU :\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum( 0 , inputs)\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0 ] = 0\n",
    "    # Softmax activation\n",
    "\n",
    "class Activation_Softmax :\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1 ,\n",
    "        keepdims = True ))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1 ,\n",
    "        keepdims = True )\n",
    "        self.output = probabilities\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "        enumerate ( zip (self.output, dvalues)):\n",
    "        # Flatten output array\n",
    "            single_output = single_output.reshape( - 1 , 1 )\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "            np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "            single_dvalues)\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        #save input and calculate / save output\n",
    "        #ot the sigmoid function\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/ (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1- self.output) * self.output\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    #Initialize optimizer - set settings,\n",
    "    #learning rate of 1 is default for this optimizer\n",
    "\n",
    "    def __init__(self, learning_rate = 1, decay = 0, momentum = 0):\n",
    "        self.name = \"optimizerSGD\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if later does not contain momentum arrays, create them\n",
    "        #filled them with zeros\n",
    "        if self.momentum:\n",
    "\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                #if there is no momentum array for weights\n",
    "                #the array does not exist for biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            #Build weight updates with momentum - take previous\n",
    "            #updates multiplied by retain factor and update with\n",
    "            #current gradients\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            #build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentum = bias_updates\n",
    "            \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad :\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__ ( self , learning_rate = 1. , decay = 0. , epsilon = 1e-7 ):\n",
    "        self.name = \"OptimizerAdagrad\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate *  ( 1. / ( 1. + self.decay * self.iterations))\n",
    "        \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr (layer, 'weight_cache' ):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights /  (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate *  layer.dbiases /       (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0, epsilon = 1e-7, rho=0.9):\n",
    "        self.name = \"RMSProp\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    \n",
    "    #Update parameters\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "\n",
    "        #if layer does not contain cache arrays\n",
    "        #create them filled with zeros\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        #update cache with squared current gradient\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache =  self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases ** 2\n",
    "\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0, epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.name = \"OptimizerAdam\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        #if layer does not contain cache arrays\n",
    "        #create them filled with zeros\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums  + (1- self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "        bias_momentums_corrected = layer.bias_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "# Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + ( 1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + ( 1 - self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "        bias_cache_corrected = layer.bias_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "\n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "\n",
    "# Common loss class\n",
    "class Loss :\n",
    "\n",
    "    #Regularization on loss calculation\n",
    "\n",
    "    def regularization_loss(self, layer):\n",
    "        #0 by default\n",
    "\n",
    "        regularization_loss = 0\n",
    "        #L1 regularization-weights\n",
    "        #calculate only when factor greater than 0\n",
    "        #L1 regularization’s penalty is the sum of all the absolute values for the weights and biases.\n",
    "        #This is a linear penalty as regularization loss returned by this function is directly proportional to\n",
    "        #parameter values. L2 regularization’s penalty is the sum of the squared weights and biases.\n",
    "\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        #L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizerl2 * np.sum(layer.weights*layer.weights)\n",
    "\n",
    "        #L1 regularization - biases\n",
    "        #calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        #L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss       \n",
    "\n",
    "        \n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate ( self , output , y ):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "        # Cross-entropy loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy ( Loss ):\n",
    "    # Forward pass\n",
    "    def forward ( self , y_pred , y_true ):\n",
    "    # Number of samples in a batch\n",
    "        samples = len (y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len (y_true.shape) == 1 :\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range (samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len (y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis = 1\n",
    "            )\n",
    "    # Losses\n",
    "        negative_log_likelihoods = - np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len (dvalues[ 0 ])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len (y_true.shape) == 1 :\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__ ( self ):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward ( self , inputs , y_true ):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len (y_true.shape) == 2 :\n",
    "            y_true = np.argmax(y_true, axis = 1 )\n",
    "            # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[ range (samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        #clip data to prevent division by 0\n",
    "        #clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        #calculate sample-wise loss\n",
    "\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped)+ (1 - y_true) * np.log(1- y_pred_clipped) )\n",
    "        sample_losses = np.mean(sample_losses, axis = 1)\n",
    "\n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #Number of samples\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        #number of outputs in every sample\n",
    "        #well use the first sample to count them\n",
    "\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        #clip data to prevent division by 0\n",
    "        #clip both sides to not drag mean towards any value\n",
    "\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "\n",
    "        #calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "\n",
    "        #Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = spiral_data( samples = 100 , classes = 2 )\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "dense1 = Layer_Dense( 2 , 512 , weight_regularizer_l1=5e-4, bias_regularizer_l2= 5e-4)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "\n",
    "dense2 = Layer_Dense( 512 ,3  )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "optimizer_class =[ \n",
    "#Optimizer_Adagrad(decay = 1e-4),\n",
    "#Optimizer_SGD(decay = 1e-3, momentum = 0.9),\n",
    "#Optimizer_RMSprop( decay = 1e-4 ),\n",
    "#Optimizer_RMSprop( learning_rate = 0.02 , decay = 1e-5 ,rho = 0.999 ),\n",
    "Optimizer_Adam( learning_rate = 0.05 , decay = 5e-5 )]\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[], \"reg_loss\":[],\"data_loss\":[]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.555 , loss: 0.697 data_loss: 0.693 , reg_loss: 0.004 , lr: 0.05 method: OptimizerAdam\n",
      "epoch: 100 , acc: 0.872 , loss: 0.414 data_loss: 0.352 , reg_loss: 0.062 , lr: 0.04975371909050202 method: OptimizerAdam\n",
      "epoch: 200 , acc: 0.945 , loss: 0.252 data_loss: 0.171 , reg_loss: 0.081 , lr: 0.049507401356502806 method: OptimizerAdam\n",
      "epoch: 300 , acc: 0.960 , loss: 0.220 data_loss: 0.125 , reg_loss: 0.095 , lr: 0.0492635105177595 method: OptimizerAdam\n",
      "epoch: 400 , acc: 0.975 , loss: 0.179 data_loss: 0.089 , reg_loss: 0.090 , lr: 0.04902201088288642 method: OptimizerAdam\n",
      "epoch: 500 , acc: 0.975 , loss: 0.166 data_loss: 0.078 , reg_loss: 0.088 , lr: 0.048782867456949125 method: OptimizerAdam\n",
      "epoch: 600 , acc: 0.980 , loss: 0.155 data_loss: 0.070 , reg_loss: 0.086 , lr: 0.04854604592455945 method: OptimizerAdam\n",
      "epoch: 700 , acc: 0.980 , loss: 0.147 data_loss: 0.064 , reg_loss: 0.083 , lr: 0.048311512633460556 method: OptimizerAdam\n",
      "epoch: 800 , acc: 0.980 , loss: 0.139 data_loss: 0.059 , reg_loss: 0.080 , lr: 0.04807923457858551 method: OptimizerAdam\n",
      "epoch: 900 , acc: 0.985 , loss: 0.134 data_loss: 0.055 , reg_loss: 0.078 , lr: 0.04784917938657352 method: OptimizerAdam\n",
      "epoch: 1000 , acc: 0.990 , loss: 0.128 data_loss: 0.052 , reg_loss: 0.076 , lr: 0.04762131530072861 method: OptimizerAdam\n",
      "epoch: 1100 , acc: 0.990 , loss: 0.123 data_loss: 0.050 , reg_loss: 0.074 , lr: 0.04739561116640599 method: OptimizerAdam\n",
      "epoch: 1200 , acc: 0.985 , loss: 0.196 data_loss: 0.073 , reg_loss: 0.123 , lr: 0.04717203641681212 method: OptimizerAdam\n",
      "epoch: 1300 , acc: 0.990 , loss: 0.143 data_loss: 0.051 , reg_loss: 0.091 , lr: 0.04695056105920466 method: OptimizerAdam\n",
      "epoch: 1400 , acc: 0.995 , loss: 0.133 data_loss: 0.049 , reg_loss: 0.084 , lr: 0.04673115566147951 method: OptimizerAdam\n",
      "epoch: 1500 , acc: 0.995 , loss: 0.126 data_loss: 0.046 , reg_loss: 0.080 , lr: 0.046513791339132055 method: OptimizerAdam\n",
      "epoch: 1600 , acc: 0.990 , loss: 0.122 data_loss: 0.046 , reg_loss: 0.076 , lr: 0.04629843974258068 method: OptimizerAdam\n",
      "epoch: 1700 , acc: 0.995 , loss: 0.118 data_loss: 0.044 , reg_loss: 0.074 , lr: 0.046085073044840774 method: OptimizerAdam\n",
      "epoch: 1800 , acc: 0.995 , loss: 0.115 data_loss: 0.043 , reg_loss: 0.072 , lr: 0.04587366392953806 method: OptimizerAdam\n",
      "epoch: 1900 , acc: 0.995 , loss: 0.111 data_loss: 0.042 , reg_loss: 0.070 , lr: 0.04566418557925019 method: OptimizerAdam\n",
      "epoch: 2000 , acc: 0.995 , loss: 0.109 data_loss: 0.041 , reg_loss: 0.068 , lr: 0.045456611664166556 method: OptimizerAdam\n",
      "epoch: 2100 , acc: 0.995 , loss: 0.107 data_loss: 0.040 , reg_loss: 0.066 , lr: 0.045250916331055706 method: OptimizerAdam\n",
      "epoch: 2200 , acc: 0.995 , loss: 0.104 data_loss: 0.039 , reg_loss: 0.065 , lr: 0.0450470741925312 method: OptimizerAdam\n",
      "epoch: 2300 , acc: 0.995 , loss: 0.103 data_loss: 0.039 , reg_loss: 0.064 , lr: 0.04484506031660612 method: OptimizerAdam\n",
      "epoch: 2400 , acc: 0.995 , loss: 0.100 data_loss: 0.038 , reg_loss: 0.062 , lr: 0.04464485021652753 method: OptimizerAdam\n",
      "epoch: 2500 , acc: 0.995 , loss: 0.099 data_loss: 0.038 , reg_loss: 0.061 , lr: 0.044446419840881816 method: OptimizerAdam\n",
      "epoch: 2600 , acc: 0.995 , loss: 0.097 data_loss: 0.037 , reg_loss: 0.060 , lr: 0.04424974556396301 method: OptimizerAdam\n",
      "epoch: 2700 , acc: 0.995 , loss: 0.095 data_loss: 0.036 , reg_loss: 0.059 , lr: 0.04405480417639544 method: OptimizerAdam\n",
      "epoch: 2800 , acc: 0.995 , loss: 0.094 data_loss: 0.036 , reg_loss: 0.058 , lr: 0.04386157287600334 method: OptimizerAdam\n",
      "epoch: 2900 , acc: 0.615 , loss: 3.222 data_loss: 3.162 , reg_loss: 0.059 , lr: 0.04367002925891961 method: OptimizerAdam\n",
      "epoch: 3000 , acc: 0.995 , loss: 0.123 data_loss: 0.040 , reg_loss: 0.082 , lr: 0.043480151310926564 method: OptimizerAdam\n",
      "epoch: 3100 , acc: 0.995 , loss: 0.110 data_loss: 0.037 , reg_loss: 0.073 , lr: 0.04329191739902161 method: OptimizerAdam\n",
      "epoch: 3200 , acc: 0.990 , loss: 0.108 data_loss: 0.039 , reg_loss: 0.069 , lr: 0.043105306263201 method: OptimizerAdam\n",
      "epoch: 3300 , acc: 0.995 , loss: 0.102 data_loss: 0.035 , reg_loss: 0.067 , lr: 0.0429202970084553 method: OptimizerAdam\n",
      "epoch: 3400 , acc: 0.995 , loss: 0.099 data_loss: 0.034 , reg_loss: 0.065 , lr: 0.04273686909696996 method: OptimizerAdam\n",
      "epoch: 3500 , acc: 0.995 , loss: 0.097 data_loss: 0.034 , reg_loss: 0.063 , lr: 0.04255500234052514 method: OptimizerAdam\n",
      "epoch: 3600 , acc: 0.995 , loss: 0.095 data_loss: 0.034 , reg_loss: 0.061 , lr: 0.042374676893088686 method: OptimizerAdam\n",
      "epoch: 3700 , acc: 0.995 , loss: 0.093 data_loss: 0.034 , reg_loss: 0.060 , lr: 0.042195873243596776 method: OptimizerAdam\n",
      "epoch: 3800 , acc: 0.995 , loss: 0.092 data_loss: 0.033 , reg_loss: 0.059 , lr: 0.04201857220891634 method: OptimizerAdam\n",
      "epoch: 3900 , acc: 0.995 , loss: 0.091 data_loss: 0.033 , reg_loss: 0.058 , lr: 0.041842754926984395 method: OptimizerAdam\n",
      "epoch: 4000 , acc: 0.995 , loss: 0.089 data_loss: 0.033 , reg_loss: 0.056 , lr: 0.04166840285011875 method: OptimizerAdam\n",
      "epoch: 4100 , acc: 0.995 , loss: 0.088 data_loss: 0.033 , reg_loss: 0.055 , lr: 0.041495497738495375 method: OptimizerAdam\n",
      "epoch: 4200 , acc: 0.995 , loss: 0.087 data_loss: 0.033 , reg_loss: 0.054 , lr: 0.041324021653787346 method: OptimizerAdam\n",
      "epoch: 4300 , acc: 0.995 , loss: 0.086 data_loss: 0.033 , reg_loss: 0.054 , lr: 0.041153956952961035 method: OptimizerAdam\n",
      "epoch: 4400 , acc: 0.995 , loss: 0.085 data_loss: 0.032 , reg_loss: 0.053 , lr: 0.040985286282224684 method: OptimizerAdam\n",
      "epoch: 4500 , acc: 0.990 , loss: 0.086 data_loss: 0.034 , reg_loss: 0.052 , lr: 0.04081799257112535 method: OptimizerAdam\n",
      "epoch: 4600 , acc: 0.995 , loss: 0.083 data_loss: 0.032 , reg_loss: 0.051 , lr: 0.04065205902678971 method: OptimizerAdam\n",
      "epoch: 4700 , acc: 0.995 , loss: 0.082 data_loss: 0.032 , reg_loss: 0.051 , lr: 0.04048746912830479 method: OptimizerAdam\n",
      "epoch: 4800 , acc: 0.995 , loss: 0.081 data_loss: 0.031 , reg_loss: 0.050 , lr: 0.04032420662123473 method: OptimizerAdam\n",
      "epoch: 4900 , acc: 0.995 , loss: 0.080 data_loss: 0.031 , reg_loss: 0.049 , lr: 0.04016225551226957 method: OptimizerAdam\n",
      "epoch: 5000 , acc: 0.995 , loss: 0.080 data_loss: 0.031 , reg_loss: 0.049 , lr: 0.04000160006400256 method: OptimizerAdam\n",
      "epoch: 5100 , acc: 0.995 , loss: 0.079 data_loss: 0.031 , reg_loss: 0.048 , lr: 0.039842224789832265 method: OptimizerAdam\n",
      "epoch: 5200 , acc: 0.995 , loss: 0.078 data_loss: 0.030 , reg_loss: 0.047 , lr: 0.03968411444898608 method: OptimizerAdam\n",
      "epoch: 5300 , acc: 0.995 , loss: 0.077 data_loss: 0.029 , reg_loss: 0.047 , lr: 0.03952725404166173 method: OptimizerAdam\n",
      "epoch: 5400 , acc: 0.670 , loss: 2.878 data_loss: 2.830 , reg_loss: 0.048 , lr: 0.03937162880428363 method: OptimizerAdam\n",
      "epoch: 5500 , acc: 0.995 , loss: 0.103 data_loss: 0.035 , reg_loss: 0.068 , lr: 0.03921722420487078 method: OptimizerAdam\n",
      "epoch: 5600 , acc: 0.995 , loss: 0.092 data_loss: 0.032 , reg_loss: 0.060 , lr: 0.03906402593851323 method: OptimizerAdam\n",
      "epoch: 5700 , acc: 0.995 , loss: 0.089 data_loss: 0.031 , reg_loss: 0.058 , lr: 0.038912019922954205 method: OptimizerAdam\n",
      "epoch: 5800 , acc: 0.995 , loss: 0.087 data_loss: 0.030 , reg_loss: 0.056 , lr: 0.038761192294274965 method: OptimizerAdam\n",
      "epoch: 5900 , acc: 0.995 , loss: 0.084 data_loss: 0.030 , reg_loss: 0.055 , lr: 0.038611529402679645 method: OptimizerAdam\n",
      "epoch: 6000 , acc: 0.995 , loss: 0.083 data_loss: 0.029 , reg_loss: 0.054 , lr: 0.03846301780837725 method: OptimizerAdam\n",
      "epoch: 6100 , acc: 0.995 , loss: 0.081 data_loss: 0.029 , reg_loss: 0.053 , lr: 0.03831564427755853 method: OptimizerAdam\n",
      "epoch: 6200 , acc: 0.995 , loss: 0.080 data_loss: 0.028 , reg_loss: 0.052 , lr: 0.03816939577846483 method: OptimizerAdam\n",
      "epoch: 6300 , acc: 0.995 , loss: 0.079 data_loss: 0.027 , reg_loss: 0.052 , lr: 0.038024259477546674 method: OptimizerAdam\n",
      "epoch: 6400 , acc: 0.995 , loss: 0.077 data_loss: 0.027 , reg_loss: 0.050 , lr: 0.03788022273570969 method: OptimizerAdam\n",
      "epoch: 6500 , acc: 0.995 , loss: 0.076 data_loss: 0.027 , reg_loss: 0.050 , lr: 0.03773727310464546 method: OptimizerAdam\n",
      "epoch: 6600 , acc: 0.990 , loss: 0.086 data_loss: 0.028 , reg_loss: 0.058 , lr: 0.03759539832324524 method: OptimizerAdam\n",
      "epoch: 6700 , acc: 0.995 , loss: 0.079 data_loss: 0.026 , reg_loss: 0.053 , lr: 0.03745458631409416 method: OptimizerAdam\n",
      "epoch: 6800 , acc: 0.995 , loss: 0.077 data_loss: 0.025 , reg_loss: 0.051 , lr: 0.03731482518004403 method: OptimizerAdam\n",
      "epoch: 6900 , acc: 0.995 , loss: 0.076 data_loss: 0.025 , reg_loss: 0.051 , lr: 0.03717610320086248 method: OptimizerAdam\n",
      "epoch: 7000 , acc: 0.995 , loss: 0.075 data_loss: 0.026 , reg_loss: 0.049 , lr: 0.03703840882995667 method: OptimizerAdam\n",
      "epoch: 7100 , acc: 0.995 , loss: 0.073 data_loss: 0.025 , reg_loss: 0.048 , lr: 0.036901730691169414 method: OptimizerAdam\n",
      "epoch: 7200 , acc: 0.995 , loss: 0.073 data_loss: 0.025 , reg_loss: 0.048 , lr: 0.03676605757564617 method: OptimizerAdam\n",
      "epoch: 7300 , acc: 0.995 , loss: 0.072 data_loss: 0.025 , reg_loss: 0.047 , lr: 0.03663137843877066 method: OptimizerAdam\n",
      "epoch: 7400 , acc: 0.995 , loss: 0.071 data_loss: 0.025 , reg_loss: 0.046 , lr: 0.03649768239716778 method: OptimizerAdam\n",
      "epoch: 7500 , acc: 0.995 , loss: 0.072 data_loss: 0.025 , reg_loss: 0.048 , lr: 0.03636495872577185 method: OptimizerAdam\n",
      "epoch: 7600 , acc: 0.995 , loss: 0.070 data_loss: 0.025 , reg_loss: 0.046 , lr: 0.03623319685495851 method: OptimizerAdam\n",
      "epoch: 7700 , acc: 0.995 , loss: 0.070 data_loss: 0.025 , reg_loss: 0.045 , lr: 0.03610238636773891 method: OptimizerAdam\n",
      "epoch: 7800 , acc: 0.995 , loss: 0.069 data_loss: 0.024 , reg_loss: 0.045 , lr: 0.03597251699701428 method: OptimizerAdam\n",
      "epoch: 7900 , acc: 0.995 , loss: 0.069 data_loss: 0.024 , reg_loss: 0.044 , lr: 0.035843578622889706 method: OptimizerAdam\n",
      "epoch: 8000 , acc: 0.995 , loss: 0.068 data_loss: 0.024 , reg_loss: 0.044 , lr: 0.03571556127004536 method: OptimizerAdam\n",
      "epoch: 8100 , acc: 0.995 , loss: 0.068 data_loss: 0.024 , reg_loss: 0.044 , lr: 0.03558845510516389 method: OptimizerAdam\n",
      "epoch: 8200 , acc: 0.995 , loss: 0.068 data_loss: 0.024 , reg_loss: 0.043 , lr: 0.03546225043441257 method: OptimizerAdam\n",
      "epoch: 8300 , acc: 0.995 , loss: 0.067 data_loss: 0.024 , reg_loss: 0.043 , lr: 0.035336937700978836 method: OptimizerAdam\n",
      "epoch: 8400 , acc: 0.995 , loss: 0.066 data_loss: 0.024 , reg_loss: 0.043 , lr: 0.03521250748265784 method: OptimizerAdam\n",
      "epoch: 8500 , acc: 0.995 , loss: 0.066 data_loss: 0.023 , reg_loss: 0.042 , lr: 0.035088950489490865 method: OptimizerAdam\n",
      "epoch: 8600 , acc: 0.995 , loss: 0.065 data_loss: 0.023 , reg_loss: 0.042 , lr: 0.0349662575614532 method: OptimizerAdam\n",
      "epoch: 8700 , acc: 0.995 , loss: 0.065 data_loss: 0.023 , reg_loss: 0.042 , lr: 0.034844419666190465 method: OptimizerAdam\n",
      "epoch: 8800 , acc: 0.995 , loss: 0.064 data_loss: 0.023 , reg_loss: 0.041 , lr: 0.034723427896801974 method: OptimizerAdam\n",
      "epoch: 8900 , acc: 0.990 , loss: 0.131 data_loss: 0.050 , reg_loss: 0.081 , lr: 0.03460327346967023 method: OptimizerAdam\n",
      "epoch: 9000 , acc: 0.995 , loss: 0.084 data_loss: 0.028 , reg_loss: 0.056 , lr: 0.034483947722335255 method: OptimizerAdam\n",
      "epoch: 9100 , acc: 0.995 , loss: 0.079 data_loss: 0.027 , reg_loss: 0.053 , lr: 0.034365442111412764 method: OptimizerAdam\n",
      "epoch: 9200 , acc: 0.995 , loss: 0.076 data_loss: 0.026 , reg_loss: 0.051 , lr: 0.03424774821055516 method: OptimizerAdam\n",
      "epoch: 9300 , acc: 0.995 , loss: 0.076 data_loss: 0.025 , reg_loss: 0.051 , lr: 0.03413085770845422 method: OptimizerAdam\n",
      "epoch: 9400 , acc: 0.995 , loss: 0.074 data_loss: 0.025 , reg_loss: 0.049 , lr: 0.034014762406884586 method: OptimizerAdam\n",
      "epoch: 9500 , acc: 0.995 , loss: 0.072 data_loss: 0.024 , reg_loss: 0.048 , lr: 0.03389945421878708 method: OptimizerAdam\n",
      "epoch: 9600 , acc: 0.995 , loss: 0.071 data_loss: 0.024 , reg_loss: 0.047 , lr: 0.033784925166390756 method: OptimizerAdam\n",
      "epoch: 9700 , acc: 0.995 , loss: 0.070 data_loss: 0.023 , reg_loss: 0.047 , lr: 0.03367116737937304 method: OptimizerAdam\n",
      "epoch: 9800 , acc: 0.995 , loss: 0.069 data_loss: 0.023 , reg_loss: 0.046 , lr: 0.033558173093056816 method: OptimizerAdam\n",
      "epoch: 9900 , acc: 0.995 , loss: 0.068 data_loss: 0.023 , reg_loss: 0.045 , lr: 0.0334459346466437 method: OptimizerAdam\n",
      "epoch: 10000 , acc: 0.995 , loss: 0.067 data_loss: 0.023 , reg_loss: 0.044 , lr: 0.03333444448148271 method: OptimizerAdam\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (300,1) (200,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Hafid\\TrainingScript\\nnfs\\Chapter15.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000003?line=75'>76</a>\u001b[0m activation1\u001b[39m.\u001b[39mforward(dense1\u001b[39m.\u001b[39moutput)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000003?line=76'>77</a>\u001b[0m dense2\u001b[39m.\u001b[39mforward(activation1\u001b[39m.\u001b[39moutput)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000003?line=78'>79</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function\u001b[39m.\u001b[39;49mforward(activation2\u001b[39m.\u001b[39;49moutput, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000003?line=80'>81</a>\u001b[0m predictions \u001b[39m=\u001b[39m (activation2\u001b[39m.\u001b[39moutput \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m ) \u001b[39m*\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000003?line=81'>82</a>\u001b[0m accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(predictions \u001b[39m==\u001b[39m y_test)\n",
      "\u001b[1;32mc:\\Hafid\\TrainingScript\\nnfs\\Chapter15.ipynb Cell 2'\u001b[0m in \u001b[0;36mLoss_BinaryCrossentropy.forward\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000001?line=415'>416</a>\u001b[0m y_pred_clipped \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(y_pred, \u001b[39m1e-7\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1e-7\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000001?line=417'>418</a>\u001b[0m \u001b[39m#calculate sample-wise loss\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000001?line=419'>420</a>\u001b[0m sample_losses \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(y_true \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mlog(y_pred_clipped)\u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_true) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m y_pred_clipped) )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000001?line=420'>421</a>\u001b[0m sample_losses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(sample_losses, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Hafid/TrainingScript/nnfs/Chapter15.ipynb#ch0000001?line=422'>423</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sample_losses\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (300,1) (200,3) "
     ]
    }
   ],
   "source": [
    "for  optimizer in optimizer_class: \n",
    "    \n",
    "    for epoch in range (10001):\n",
    "        \n",
    "\n",
    "        dense1.forward(X)\n",
    "        #this is whhen input * weight + bias\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "\n",
    "        activation1.forward(dense1.output) #this is relu\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "\n",
    "\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "        dense2.forward(activation1.output) #this is input * weight + bias\n",
    "\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        data_loss = loss_function.calculate(activation2.output, y)\n",
    "        \n",
    "        #calculate regularization penalty\n",
    "\n",
    "        regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "        #calculate overall loss\n",
    "        loss = data_loss + regularization_loss\n",
    "\n",
    "        predictions = (activation2.output > 0.5) * 1\n",
    "        accuracy = np.mean(predictions == y)\n",
    "\n",
    "        if not epoch % 100:\n",
    "        \n",
    "\n",
    "            print(f'epoch: {epoch} , ' +\n",
    "                    f'acc: {accuracy:.3f} , ' +\n",
    "                    f'loss: {loss:.3f} ' +\n",
    "                    f'data_loss: {data_loss :.3f} , ' +\n",
    "                    f'reg_loss: {regularization_loss :.3f} , ' +\n",
    "                    f'lr: {optimizer.current_learning_rate} ' +\n",
    "                    f'method: {optimizer.name}')\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"data_loss\"].append(data_loss)\n",
    "            scores[\"reg_loss\"].append(regularization_loss)\n",
    "            scores[\"lr\"].append(optimizer.current_learning_rate)       \n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(activation2.output, y)\n",
    "        activation2.backward(loss_function.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "            \n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()\n",
    "#Validate the model\n",
    "\n",
    "#create test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.995, loss 0.023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test, y_test = spiral_data(samples = 100, classes =2)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "predictions = (activation2.output > 0.5 ) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss {loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(data = scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dropout_rate = 0.5\n",
    "# Example output containing 10 values\n",
    "example_output = [ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 , - 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ]\n",
    "\n",
    "while True:\n",
    "    #Randomly choose inde and set value to 0\n",
    "\n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index ] =0 \n",
    "    \n",
    "    #we might set an index that already is zeroed\n",
    "    #there are different ways of overcoming this problem\n",
    "    #for simplicity we count values that are exactly 0\n",
    "    #while it's extremely rare in real model that weights\n",
    "    #are exacly 0, this is not the best method for sure\n",
    "\n",
    "    dropped_out = 0\n",
    "\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out += 1\n",
    "\n",
    "    #if required number of outputs is zeroed - leave the loop\n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    "\n",
    "print(example_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint( 0 , len (example_output) - 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial( 2 , 0.5 , size = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.20\n",
    "np.random.binomial( 1 , 1 - dropout_rate, size = 5 )\n",
    "\n",
    "#this means that there are 80 % that it can be True for once toss. The results are F, T, T, F , F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dropout_rate = 0.3\n",
    "example_output = np.array([ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 ,\n",
    "- 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ])\n",
    "example_output *= np.random.binomial( 1 , 1 - dropout_rate,\n",
    "example_output.shape)\n",
    "print (example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dropout_rate = 0.2\n",
    "example_output = np.array([ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 ,- 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ])\n",
    "print (f'sum initial { sum (example_output)}')\n",
    "sums = []\n",
    "for i in range ( 1000000 ):\n",
    "    example_output2 = example_output *  np.random.binomial( 1 , 1 - dropout_rate, example_output.shape) /    ( 1 - dropout_rate)\n",
    "    sums.append( sum (example_output2))\n",
    "print ( f'mean sum: {np.mean(sums)} ' )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24e68ee5c3be1453adda2c12808546301ced8adfec32f06d9e3ee889c2d5adb6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
