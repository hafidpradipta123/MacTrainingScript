{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import pandas as pd\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1, decay  = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.name = \"OptimizerSGD\"\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  \n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01088035 -0.00576474 -0.00763971  0.00521755  0.0156246  -0.00627874\n",
      "  -0.00234849  0.00358712  0.00734204 -0.0026589  -0.01508541  0.00540999\n",
      "  -0.00292673 -0.00465327  0.00507049 -0.00428421  0.00083785 -0.00651471\n",
      "  -0.00359756  0.01991302 -0.01041127  0.00683203  0.01624429 -0.00524556\n",
      "  -0.00191684  0.02008922  0.01214893  0.00211716 -0.02711518 -0.0019272\n",
      "  -0.01231443 -0.0033329   0.0022073   0.02063003 -0.0065669  -0.00341786\n",
      "   0.00405571 -0.00123952  0.00797736 -0.00290438 -0.00479556  0.01427437\n",
      "  -0.00063681  0.00085512 -0.01047926 -0.01134318 -0.00287991 -0.01435509\n",
      "  -0.01259352  0.01274262 -0.00736043 -0.00298008 -0.00969414  0.00980254\n",
      "   0.01071508 -0.01197378 -0.00512049 -0.00387147  0.0126904  -0.00065774\n",
      "   0.01066583  0.0116914   0.00666894  0.01452146]\n",
      " [-0.00432057 -0.01720823  0.0297555  -0.00095595 -0.00075643 -0.00325404\n",
      "   0.00014532  0.0037986  -0.00945634 -0.02635358 -0.014105   -0.0020406\n",
      "  -0.00845483 -0.01583974  0.00116683  0.00177844  0.01556313  0.01382934\n",
      "  -0.00976482  0.00136011 -0.00425067  0.00706354 -0.00953616 -0.00543833\n",
      "   0.01593576 -0.0027197  -0.00034101  0.00347282  0.00745575  0.01876775\n",
      "   0.02342509  0.01191437 -0.00994065 -0.00792372 -0.00227901 -0.00132486\n",
      "   0.00962912 -0.00891002  0.00675716 -0.00201844 -0.00124839 -0.01231272\n",
      "  -0.00679897 -0.01823927 -0.00478189  0.00119693  0.0015616  -0.01179689\n",
      "  -0.00230096 -0.00447026  0.00291974  0.01434205  0.00888518 -0.00817442\n",
      "  -0.00432582  0.00891368 -0.01524128 -0.00148223  0.01946438  0.00837395\n",
      "  -0.0032884  -0.0127551  -0.01219443 -0.03093137]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer_class = [Optimizer_SGD( learning_rate= 1),\n",
    "Optimizer_SGD( learning_rate= 0.85),\n",
    "Optimizer_SGD( decay = 1e-2),\n",
    "Optimizer_SGD( decay = 1e-3)\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[],\"decay\":[]}\n",
    "\n",
    "for optimizer in optimizer_class:\n",
    "    dense1 = Layer_Dense( 2 , 64 )\n",
    "    dense2 = Layer_Dense( 64 , 3 )\n",
    "    for epoch in range ( 10001 ):\n",
    "        # Perform a forward pass of our training data through this layer\n",
    "        dense1.forward(X)\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "        activation1.forward(dense1.output)\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "        dense2.forward(activation1.output)\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "        loss = loss_activation.forward(dense2.output, y)\n",
    "        # Calculate accuracy from output of activation2 and targets\n",
    "        # calculate values along first axis\n",
    "        predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "        if len (y.shape) == 2 :\n",
    "            y = np.argmax(y, axis = 1 )\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        if not epoch % 10000 :\n",
    "            print (f'epoch: {epoch} , ' +\n",
    "                    f'acc: {accuracy :.3f} , ' +\n",
    "                    f'loss: {loss :.3f},' +\n",
    "                    f'lr:{optimizer.current_learning_rate:.3f}' )\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"lr\"].append(optimizer.current_learning_rate)\n",
    "            scores[\"decay\"].append(optimizer.current_decay)    \n",
    "                # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        # Update weights and biases\n",
    "        optimizer.pre_udpate_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(data = scores)\n",
    "df_scores[df_scores.epoch !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Optimizer_SGD()\n",
    "print(test.decay)\n",
    "print(test.iterations)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
