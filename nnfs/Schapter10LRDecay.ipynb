{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import pandas as pd\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1, decay  = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.name = \"OptimizerSGD\"\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1./ + self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1./ + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  \n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.390 , loss: 1.099,lr:1.000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Optimizer_SGD' object has no attribute 'iterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_21774/3399967530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Update weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_udpate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_21774/2815736824.py\u001b[0m in \u001b[0;36mpre_udpate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_udpate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Optimizer_SGD' object has no attribute 'iterations'"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer_class = [Optimizer_SGD( decay = 1e-2),\n",
    "Optimizer_SGD( decay = 1e-3)\n",
    "]\n",
    "\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[],\"decay\":[]}\n",
    "\n",
    "for optimizer in optimizer_class:\n",
    "    for epoch in range ( 10001 ):\n",
    "        # Perform a forward pass of our training data through this layer\n",
    "        dense1.forward(X)\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "        activation1.forward(dense1.output)\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "        dense2.forward(activation1.output)\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "        loss = loss_activation.forward(dense2.output, y)\n",
    "        # Calculate accuracy from output of activation2 and targets\n",
    "        # calculate values along first axis\n",
    "        predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "        if len (y.shape) == 2 :\n",
    "            y = np.argmax(y, axis = 1 )\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        if not epoch % 1000 :\n",
    "            print (f'epoch: {epoch} , ' +\n",
    "                    f'acc: {accuracy :.3f} , ' +\n",
    "                    f'loss: {loss :.3f},' +\n",
    "                    f'lr:{optimizer.current_learning_rate:.3f}' )\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"lr\"].append(optimizer.current_learning_rate)\n",
    "            scores[\"decay\"].append(optimizer.decay)    \n",
    "                # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        # Update weights and biases\n",
    "        optimizer.pre_udpate_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(data = scores)\n",
    "df_scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
