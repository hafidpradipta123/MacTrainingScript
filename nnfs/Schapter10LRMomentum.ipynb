{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import pandas as pd\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1., decay  = 0., momentum  = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.name = f'OptimizerSGD lr:{self.learning_rate}, decay ={self.decay}, momentum = {self.momentum}'\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "        #if we use momentum\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases  \n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    def __init__(self, learning_rate = 1, decay  = 0.,  epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'Adagrad decay =  {self.decay}'\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases  / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self, learning_rate = 0.001, decay  = 0., rho = 0.9, epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'RMSProp =  {self.decay}'\n",
    "        self.rho = rho\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + ( 1- self.rho) * layer.dweights **2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + ( 1- self.rho) * layer.dbiases **2\n",
    "\n",
    "\n",
    "        layer.weights += -self.learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate = 0.001, decay  = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'OptimizerAdam lr =  {self.learning_rate}, decay = {self.current_decay}'\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1-self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1- self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1- self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1- self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        #get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1- self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1- self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "            \n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.343 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.677 , loss: 0.729,lr:1.000\n",
      "epoch: 0 , acc: 0.313 , loss: 1.099,lr:0.850\n",
      "epoch: 10000 , acc: 0.760 , loss: 0.561,lr:0.850\n",
      "epoch: 0 , acc: 0.367 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.703 , loss: 0.644,lr:0.010\n",
      "epoch: 0 , acc: 0.410 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.570 , loss: 0.922,lr:0.091\n",
      "epoch: 0 , acc: 0.383 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.810 , loss: 0.440,lr:0.091\n",
      "epoch: 0 , acc: 0.357 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.947 , loss: 0.136,lr:0.091\n",
      "epoch: 0 , acc: 0.343 , loss: 1.099,lr:1.000\n",
      "epoch: 10000 , acc: 0.840 , loss: 0.350,lr:0.500\n",
      "epoch: 0 , acc: 0.380 , loss: 1.099,lr:0.001\n",
      "epoch: 10000 , acc: 0.693 , loss: 0.692,lr:0.001\n",
      "epoch: 0 , acc: 0.307 , loss: 1.099,lr:0.050\n",
      "epoch: 10000 , acc: 0.923 , loss: 0.169,lr:0.050\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer_class = [\n",
    "Optimizer_SGD( learning_rate= 1),\n",
    "Optimizer_SGD( learning_rate= 0.85),\n",
    "Optimizer_SGD( decay = 1e-2),\n",
    "Optimizer_SGD( decay = 1e-3),\n",
    "Optimizer_SGD(decay = 1e-3,momentum = 0.5),\n",
    "Optimizer_SGD(decay = 1e-3,momentum = 0.9),\n",
    "Optimizer_Adagrad(decay = 1e-4),\n",
    "Optimizer_RMSprop(decay = 1e-4),\n",
    "Optimizer_Adam(learning_rate = 0.05,decay = 5e-7)\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[],\"decay\":[]}\n",
    "\n",
    "for optimizer in optimizer_class:\n",
    "    dense1 = Layer_Dense( 2 , 64 )\n",
    "    activation1 = Activation_ReLU()\n",
    "    dense2 = Layer_Dense( 64 , 3 )\n",
    "    for epoch in range ( 10001 ):\n",
    "        # Perform a forward pass of our training data through this layer\n",
    "        dense1.forward(X)\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "        activation1.forward(dense1.output)\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "        dense2.forward(activation1.output)\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "        loss = loss_activation.forward(dense2.output, y)\n",
    "        # Calculate accuracy from output of activation2 and targets\n",
    "        # calculate values along first axis\n",
    "        predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "        if len (y.shape) == 2 :\n",
    "            y = np.argmax(y, axis = 1 )\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        if not epoch % 10000 :\n",
    "            print (f'epoch: {epoch} , ' +\n",
    "                    f'acc: {accuracy :.3f} , ' +\n",
    "                    f'loss: {loss :.3f},' +\n",
    "                    f'lr:{optimizer.current_learning_rate:.3f}' )\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"lr\"].append(optimizer.current_learning_rate)\n",
    "            scores[\"decay\"].append(optimizer.current_decay)    \n",
    "                # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        # Update weights and biases\n",
    "        optimizer.pre_udpate_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>acc</th>\n",
       "      <th>decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OptimizerSGD lr:1, decay =0.0, momentum = 0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.729148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OptimizerSGD lr:0.85, decay =0.0, momentum = 0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.560901</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OptimizerSGD lr:1.0, decay =0.01, momentum = 0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.643755</td>\n",
       "      <td>0.009902</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.009902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OptimizerSGD lr:1.0, decay =0.001, momentum = 0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.921615</td>\n",
       "      <td>0.090917</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.090917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OptimizerSGD lr:1.0, decay =0.001, momentum = 0.5</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.439945</td>\n",
       "      <td>0.090917</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.090917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OptimizerSGD lr:1.0, decay =0.001, momentum = 0.9</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.135620</td>\n",
       "      <td>0.090917</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.090917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Adagrad decay =  0.0001</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.350090</td>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RMSProp =  0.0001</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.691779</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OptimizerAdam lr =  0.05, decay = 5e-07</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.168533</td>\n",
       "      <td>0.049751</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.995025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               method  epoch      loss  \\\n",
       "1         OptimizerSGD lr:1, decay =0.0, momentum = 0  10000  0.729148   \n",
       "3      OptimizerSGD lr:0.85, decay =0.0, momentum = 0  10000  0.560901   \n",
       "5      OptimizerSGD lr:1.0, decay =0.01, momentum = 0  10000  0.643755   \n",
       "7     OptimizerSGD lr:1.0, decay =0.001, momentum = 0  10000  0.921615   \n",
       "9   OptimizerSGD lr:1.0, decay =0.001, momentum = 0.5  10000  0.439945   \n",
       "11  OptimizerSGD lr:1.0, decay =0.001, momentum = 0.9  10000  0.135620   \n",
       "13                            Adagrad decay =  0.0001  10000  0.350090   \n",
       "15                                  RMSProp =  0.0001  10000  0.691779   \n",
       "17            OptimizerAdam lr =  0.05, decay = 5e-07  10000  0.168533   \n",
       "\n",
       "          lr       acc     decay  \n",
       "1   1.000000  0.676667  0.000000  \n",
       "3   0.850000  0.760000  0.000000  \n",
       "5   0.009902  0.703333  0.009902  \n",
       "7   0.090917  0.570000  0.090917  \n",
       "9   0.090917  0.810000  0.090917  \n",
       "11  0.090917  0.946667  0.090917  \n",
       "13  0.500025  0.840000  0.500025  \n",
       "15  0.000500  0.693333  0.500025  \n",
       "17  0.049751  0.923333  0.995025  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.DataFrame(data = scores)\n",
    "df_scores[df_scores.epoch !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test = Optimizer_SGD()\n",
    "print(test.decay)\n",
    "print(test.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test.momentum# Create dataset\\nX, y = spiral_data( samples = 100 , classes = 3 )\\n# Create Dense layer with 2 input features and 64 output values\\ndense1 = Layer_Dense( 2 , 64 )\\n# Create ReLU activation (to be used with Dense layer):\\nactivation1 = Activation_ReLU()\\n# Create second Dense layer with 64 input features (as we take output\\n# of previous layer here) and 3 output values (output values)\\ndense2 = Layer_Dense( 64 , 3 )\\n# Create Softmax classifier's combined loss and activation\\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\\n# Create optimizer\\n\\noptimizer =Optimizer_SGD(decay = 1e-3,momentum = 0.5)\\n\\nfor epoch in range ( 11):\\n    # Perform a forward pass of our training data through this layer\\n    dense1.forward(X)\\n    # Perform a forward pass through activation function\\n    # takes the output of first dense layer here\\n    activation1.forward(dense1.output)\\n    # Perform a forward pass through second Dense layer\\n    # takes outputs of activation function of first layer as inputs\\n    dense2.forward(activation1.output)\\n    # Perform a forward pass through the activation/loss function\\n    # takes the output of second dense layer here and returns loss\\n    loss = loss_activation.forward(dense2.output, y)\\n    # Calculate accuracy from output of activation2 and targets\\n    # calculate values along first axis\\n    predictions = np.argmax(loss_activation.output, axis = 1 )\\n    if len (y.shape) == 2 :\\n        y = np.argmax(y, axis = 1 )\\n    accuracy = np.mean(predictions == y)\\n    if not epoch % 1 :\\n        print (f'epoch: {epoch} , ' +\\n                f'acc: {accuracy :.3f} , ' +\\n                f'loss: {loss :.3f},' +\\n                f'lr:{optimizer.current_learning_rate:.3f}' )   \\n            # Backward pass\\n    loss_activation.backward(loss_activation.output, y)\\n    dense2.backward(loss_activation.dinputs)\\n    activation1.backward(dense2.dinputs)\\n    dense1.backward(activation1.dinputs)\\n    # Update weights and biases\\n    print(f'optimizer momentum ={optimizer.momentum}')\\n\\n\\n    optimizer.pre_udpate_params()\\n    print(f'decay = {optimizer.current_decay}')\\n    print(f'clr: {optimizer.current_learning_rate}')\\n    optimizer.update_params(dense1)\\n    print(dense1.weight_momentums[:2,:5])\\n    optimizer.update_params(dense2)\\n    optimizer.post_update_params()\\n    \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''test.momentum# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "\n",
    "optimizer =Optimizer_SGD(decay = 1e-3,momentum = 0.5)\n",
    "\n",
    "for epoch in range ( 11):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if not epoch % 1 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "                f'acc: {accuracy :.3f} , ' +\n",
    "                f'loss: {loss :.3f},' +\n",
    "                f'lr:{optimizer.current_learning_rate:.3f}' )   \n",
    "            # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    print(f'optimizer momentum ={optimizer.momentum}')\n",
    "\n",
    "\n",
    "    optimizer.pre_udpate_params()\n",
    "    print(f'decay = {optimizer.current_decay}')\n",
    "    print(f'clr: {optimizer.current_learning_rate}')\n",
    "    optimizer.update_params(dense1)\n",
    "    print(dense1.weight_momentums[:2,:5])\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "weights:(2, 64)\n",
      " weight momentum :(2, 64)\n",
      "after:[[ 0.00000000e+00 -4.17712674e-05  1.97196539e-04 -1.15758173e-04\n",
      "  -5.40050515e-05]\n",
      " [ 0.00000000e+00 -3.10440995e-05 -1.64139961e-04 -1.14780413e-04\n",
      "  -7.17771644e-05]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Optimizer_Adam' object has no attribute 'momentum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_28074/155066874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after:{dense1.weight_momentums[:2,:5]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mweight_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_momentums\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_learning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_momentums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Optimizer_Adam' object has no attribute 'momentum'"
     ]
    }
   ],
   "source": [
    "dense1.weight_momentums = np.zeros_like(dense1.weights)\n",
    "print(dense1.weight_momentums[:2,:5])\n",
    "print(f'weights:{dense1.weights.shape}')\n",
    "print(f' weight momentum :{dense1.weight_momentums.shape}')\n",
    "optimizer.update_params(dense1)\n",
    "print(f'after:{dense1.weight_momentums[:2,:5]}')\n",
    "weight_updates = optimizer.momentum * dense1.weight_momentums - optimizer.current_learning_rate * dense1.dweights\n",
    "dense1.weight_momentums = weight_updates"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
