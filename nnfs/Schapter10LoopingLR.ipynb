{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import pandas as pd\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = \"OptimizerSGD\"\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.397 , loss: 1.099 \n",
      "epoch: 1000 , acc: 0.457 , loss: 1.046 \n",
      "epoch: 2000 , acc: 0.537 , loss: 0.897 \n",
      "epoch: 3000 , acc: 0.637 , loss: 0.739 \n",
      "epoch: 4000 , acc: 0.723 , loss: 0.604 \n",
      "epoch: 5000 , acc: 0.767 , loss: 0.552 \n",
      "epoch: 6000 , acc: 0.790 , loss: 0.506 \n",
      "epoch: 7000 , acc: 0.803 , loss: 0.477 \n",
      "epoch: 8000 , acc: 0.737 , loss: 0.763 \n",
      "epoch: 9000 , acc: 0.820 , loss: 0.449 \n",
      "epoch: 10000 , acc: 0.830 , loss: 0.428 \n",
      "epoch: 0 , acc: 0.837 , loss: 0.439 \n",
      "epoch: 1000 , acc: 0.840 , loss: 0.420 \n",
      "epoch: 2000 , acc: 0.853 , loss: 0.411 \n",
      "epoch: 3000 , acc: 0.850 , loss: 0.402 \n",
      "epoch: 4000 , acc: 0.783 , loss: 0.817 \n",
      "epoch: 5000 , acc: 0.863 , loss: 0.384 \n",
      "epoch: 6000 , acc: 0.857 , loss: 0.387 \n",
      "epoch: 7000 , acc: 0.857 , loss: 0.379 \n",
      "epoch: 8000 , acc: 0.870 , loss: 0.374 \n",
      "epoch: 9000 , acc: 0.697 , loss: 1.354 \n",
      "epoch: 10000 , acc: 0.870 , loss: 0.359 \n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer_class = [Optimizer_SGD( learning_rate = 1),\n",
    "Optimizer_SGD( learning_rate = 0.85)\n",
    "]\n",
    "\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[]}\n",
    "\n",
    "for optimizer in optimizer_class:\n",
    "    for epoch in range ( 10001 ):\n",
    "        # Perform a forward pass of our training data through this layer\n",
    "        dense1.forward(X)\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "        activation1.forward(dense1.output)\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "        dense2.forward(activation1.output)\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "        loss = loss_activation.forward(dense2.output, y)\n",
    "        # Calculate accuracy from output of activation2 and targets\n",
    "        # calculate values along first axis\n",
    "        predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "        if len (y.shape) == 2 :\n",
    "            y = np.argmax(y, axis = 1 )\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        if not epoch % 1000 :\n",
    "            print (f'epoch: {epoch} , ' +\n",
    "            f'acc: {accuracy :.3f} , ' +\n",
    "            f'loss: {loss :.3f} ' )\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"lr\"].append(optimizer.learning_rate)    \n",
    "                # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>0</td>\n",
       "      <td>1.098605</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.396667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>1000</td>\n",
       "      <td>1.045727</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.738755</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.604224</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.723333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.552417</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.505868</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.476831</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.803333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.762996</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.736667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>9000</td>\n",
       "      <td>0.449063</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.427541</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.836667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.419709</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.410558</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.402398</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.816693</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.383848</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.863333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.387183</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>7000</td>\n",
       "      <td>0.379293</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.856667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>8000</td>\n",
       "      <td>0.374094</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>9000</td>\n",
       "      <td>1.353961</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>OptimizerSGD</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.359255</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          method  epoch      loss    lr       acc\n",
       "0   OptimizerSGD      0  1.098605  1.00  0.396667\n",
       "1   OptimizerSGD   1000  1.045727  1.00  0.456667\n",
       "2   OptimizerSGD   2000  0.896628  1.00  0.536667\n",
       "3   OptimizerSGD   3000  0.738755  1.00  0.636667\n",
       "4   OptimizerSGD   4000  0.604224  1.00  0.723333\n",
       "5   OptimizerSGD   5000  0.552417  1.00  0.766667\n",
       "6   OptimizerSGD   6000  0.505868  1.00  0.790000\n",
       "7   OptimizerSGD   7000  0.476831  1.00  0.803333\n",
       "8   OptimizerSGD   8000  0.762996  1.00  0.736667\n",
       "9   OptimizerSGD   9000  0.449063  1.00  0.820000\n",
       "10  OptimizerSGD  10000  0.427541  1.00  0.830000\n",
       "11  OptimizerSGD      0  0.439404  0.85  0.836667\n",
       "12  OptimizerSGD   1000  0.419709  0.85  0.840000\n",
       "13  OptimizerSGD   2000  0.410558  0.85  0.853333\n",
       "14  OptimizerSGD   3000  0.402398  0.85  0.850000\n",
       "15  OptimizerSGD   4000  0.816693  0.85  0.783333\n",
       "16  OptimizerSGD   5000  0.383848  0.85  0.863333\n",
       "17  OptimizerSGD   6000  0.387183  0.85  0.856667\n",
       "18  OptimizerSGD   7000  0.379293  0.85  0.856667\n",
       "19  OptimizerSGD   8000  0.374094  0.85  0.870000\n",
       "20  OptimizerSGD   9000  1.353961  0.85  0.696667\n",
       "21  OptimizerSGD  10000  0.359255  0.85  0.870000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.DataFrame(data = scores)\n",
    "df_scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
