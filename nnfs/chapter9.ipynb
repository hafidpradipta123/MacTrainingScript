{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "# Dense layer\n",
    "class Layer_Dense :\n",
    "# Layer initialization\n",
    "    def __init__ ( self , n_inputs , n_neurons ):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros(( 1 , n_neurons))\n",
    "        # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0 , keepdims = True )\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "    # ReLU activation\n",
    "class Activation_ReLU :\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum( 0 , inputs)\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0 ] = 0\n",
    "    # Softmax activation\n",
    "class Activation_Softmax :\n",
    "    # Forward pass\n",
    "    def forward ( self , inputs ):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1 ,\n",
    "        keepdims = True ))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1 ,\n",
    "        keepdims = True )\n",
    "        self.output = probabilities\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues ):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "        enumerate ( zip (self.output, dvalues)):\n",
    "        # Flatten output array\n",
    "            single_output = single_output.reshape( - 1 , 1 )\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "            np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "            single_dvalues)\n",
    "            # Common loss class\n",
    "class Loss :\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate ( self , output , y ):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "        # Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy ( Loss ):\n",
    "    # Forward pass\n",
    "    def forward ( self , y_pred , y_true ):\n",
    "    # Number of samples in a batch\n",
    "        samples = len (y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len (y_true.shape) == 1 :\n",
    "            correct_confidences = y_pred_clipped[\n",
    "            range (samples),\n",
    "            y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len (y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis = 1\n",
    "            )\n",
    "    # Losses\n",
    "        negative_log_likelihoods = - np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "        # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len (dvalues[ 0 ])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len (y_true.shape) == 1 :\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__ ( self ):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward ( self , inputs , y_true ):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward ( self , dvalues , y_true ):\n",
    "        # Number of samples\n",
    "        samples = len (dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len (y_true.shape) == 2 :\n",
    "            y_true = np.argmax(y_true, axis = 1 )\n",
    "            # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[ range (samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Basic_SGD:\n",
    "\n",
    "    def __init__(self, learning_rate = 1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    #Initialize optimizer - set settings,\n",
    "    #learning rate of 1 is default for this optimizer\n",
    "\n",
    "    def __init__(self, learning_rate = 1, decay = 0, momentum = 0):\n",
    "        self.name = \"optimizerSGD\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if later does not contain momentum arrays, create them\n",
    "        #filled them with zeros\n",
    "        if self.momentum:\n",
    "\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                #if there is no momentum array for weights\n",
    "                #the array does not exist for biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            #Build weight updates with momentum - take previous\n",
    "            #updates multiplied by retain factor and update with\n",
    "            #current gradients\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            #build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentum = bias_updates\n",
    "            \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad :\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__ ( self , learning_rate = 1. , decay = 0. , epsilon = 1e-7 ):\n",
    "        self.name = \"OptimizerAdagrad\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params ( self ):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate *  ( 1. / ( 1. + self.decay * self.iterations))\n",
    "        \n",
    "    # Update parameters\n",
    "    def update_params ( self , layer ):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr (layer, 'weight_cache' ):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights /  (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate *  layer.dbiases /       (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        # Call once after any parameter updates\n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0, epsilon = 1e-7, rho=0.9):\n",
    "        self.name = \"RMSProp\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    \n",
    "    #Update parameters\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "\n",
    "        #if layer does not contain cache arrays\n",
    "        #create them filled with zeros\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        #update cache with squared current gradient\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache =  self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases ** 2\n",
    "\n",
    "        layer.weights += - self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0, epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.name = \"OptimizerAdam\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        #if layer does not contain cache arrays\n",
    "        #create them filled with zeros\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums  + (1- self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "        bias_momentums_corrected = layer.bias_momentums / ( 1 - self.beta_1 ** (self.iterations + 1 ))\n",
    "# Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + ( 1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + ( 1 - self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "        bias_cache_corrected = layer.bias_cache / ( 1 - self.beta_2 ** (self.iterations + 1 ))\n",
    "\n",
    "        layer.weights += - self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += - self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params ( self ):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "\n",
    "dense2 = Layer_Dense( 64 ,3  )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "optimizer_class =[ \n",
    "Optimizer_Adagrad(decay = 1e-4),\n",
    "Optimizer_SGD(decay = 1e-3, momentum = 0.9),\n",
    "Optimizer_RMSprop( decay = 1e-4 ),\n",
    "Optimizer_RMSprop( learning_rate = 0.02 , decay = 1e-5 ,rho = 0.999 ),\n",
    "Optimizer_Adam( learning_rate = 0.02 , decay = 1e-5 )]\n",
    "\n",
    "scores = {\"method\":[],\"epoch\":[],\"loss\":[],\"lr\":[],\"acc\":[] }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss:1.099, lr: 1.0, method: OptimizerAdagrad\n",
      "epoch: 1000, acc: 0.690, loss:0.670, lr: 0.9091735612328392, method: OptimizerAdagrad\n",
      "epoch: 2000, acc: 0.787, loss:0.556, lr: 0.8334027835652972, method: OptimizerAdagrad\n",
      "epoch: 3000, acc: 0.780, loss:0.508, lr: 0.7692899453804138, method: OptimizerAdagrad\n",
      "epoch: 4000, acc: 0.807, loss:0.478, lr: 0.7143367383384527, method: OptimizerAdagrad\n",
      "epoch: 5000, acc: 0.813, loss:0.459, lr: 0.6667111140742716, method: OptimizerAdagrad\n",
      "epoch: 6000, acc: 0.837, loss:0.436, lr: 0.6250390649415589, method: OptimizerAdagrad\n",
      "epoch: 7000, acc: 0.837, loss:0.417, lr: 0.5882698982293076, method: OptimizerAdagrad\n",
      "epoch: 8000, acc: 0.847, loss:0.404, lr: 0.5555864214678593, method: OptimizerAdagrad\n",
      "epoch: 9000, acc: 0.840, loss:0.393, lr: 0.5263434917627243, method: OptimizerAdagrad\n",
      "epoch: 10000, acc: 0.840, loss:0.385, lr: 0.5000250012500626, method: OptimizerAdagrad\n",
      "epoch: 0, acc: 0.837, loss:0.385, lr: 1, method: optimizerSGD\n",
      "epoch: 1000, acc: 0.717, loss:0.607, lr: 0.5002501250625312, method: optimizerSGD\n",
      "epoch: 2000, acc: 0.760, loss:0.567, lr: 0.33344448149383127, method: optimizerSGD\n",
      "epoch: 3000, acc: 0.737, loss:0.536, lr: 0.25006251562890724, method: optimizerSGD\n",
      "epoch: 4000, acc: 0.790, loss:0.434, lr: 0.2000400080016003, method: optimizerSGD\n",
      "epoch: 5000, acc: 0.823, loss:0.391, lr: 0.16669444907484582, method: optimizerSGD\n",
      "epoch: 6000, acc: 0.833, loss:0.369, lr: 0.1428775539362766, method: optimizerSGD\n",
      "epoch: 7000, acc: 0.833, loss:0.359, lr: 0.12501562695336915, method: optimizerSGD\n",
      "epoch: 8000, acc: 0.823, loss:0.356, lr: 0.11112345816201799, method: optimizerSGD\n",
      "epoch: 9000, acc: 0.857, loss:0.340, lr: 0.1000100010001, method: optimizerSGD\n",
      "epoch: 10000, acc: 0.850, loss:0.373, lr: 0.09091735612328393, method: optimizerSGD\n",
      "epoch: 0, acc: 0.840, loss:0.369, lr: 0.001, method: RMSProp\n",
      "epoch: 1000, acc: 0.903, loss:0.225, lr: 0.0009091735612328393, method: RMSProp\n",
      "epoch: 2000, acc: 0.917, loss:0.207, lr: 0.0008334027835652972, method: RMSProp\n",
      "epoch: 3000, acc: 0.920, loss:0.192, lr: 0.0007692899453804139, method: RMSProp\n",
      "epoch: 4000, acc: 0.930, loss:0.181, lr: 0.0007143367383384526, method: RMSProp\n",
      "epoch: 5000, acc: 0.927, loss:0.173, lr: 0.0006667111140742717, method: RMSProp\n",
      "epoch: 6000, acc: 0.933, loss:0.168, lr: 0.0006250390649415589, method: RMSProp\n",
      "epoch: 7000, acc: 0.937, loss:0.164, lr: 0.0005882698982293077, method: RMSProp\n",
      "epoch: 8000, acc: 0.937, loss:0.160, lr: 0.0005555864214678594, method: RMSProp\n",
      "epoch: 9000, acc: 0.937, loss:0.157, lr: 0.0005263434917627244, method: RMSProp\n",
      "epoch: 10000, acc: 0.937, loss:0.154, lr: 0.0005000250012500625, method: RMSProp\n",
      "epoch: 0, acc: 0.937, loss:0.154, lr: 0.02, method: RMSProp\n",
      "epoch: 1000, acc: 0.930, loss:0.172, lr: 0.019802176259170884, method: RMSProp\n",
      "epoch: 2000, acc: 0.940, loss:0.151, lr: 0.019608035372895814, method: RMSProp\n",
      "epoch: 3000, acc: 0.937, loss:0.148, lr: 0.01941766424916747, method: RMSProp\n",
      "epoch: 4000, acc: 0.950, loss:0.142, lr: 0.019230954143789846, method: RMSProp\n",
      "epoch: 5000, acc: 0.943, loss:0.138, lr: 0.01904780045524243, method: RMSProp\n",
      "epoch: 6000, acc: 0.943, loss:0.143, lr: 0.018868102529269144, method: RMSProp\n",
      "epoch: 7000, acc: 0.947, loss:0.140, lr: 0.018691763474424996, method: RMSProp\n",
      "epoch: 8000, acc: 0.947, loss:0.130, lr: 0.01851868998787026, method: RMSProp\n",
      "epoch: 9000, acc: 0.943, loss:0.127, lr: 0.018348792190754044, method: RMSProp\n",
      "epoch: 10000, acc: 0.950, loss:0.134, lr: 0.018181983472577025, method: RMSProp\n",
      "epoch: 0, acc: 0.940, loss:0.136, lr: 0.02, method: OptimizerAdam\n",
      "epoch: 1000, acc: 0.947, loss:0.125, lr: 0.019802176259170884, method: OptimizerAdam\n",
      "epoch: 2000, acc: 0.947, loss:0.122, lr: 0.019608035372895814, method: OptimizerAdam\n",
      "epoch: 3000, acc: 0.947, loss:0.119, lr: 0.01941766424916747, method: OptimizerAdam\n",
      "epoch: 4000, acc: 0.947, loss:0.117, lr: 0.019230954143789846, method: OptimizerAdam\n",
      "epoch: 5000, acc: 0.947, loss:0.115, lr: 0.01904780045524243, method: OptimizerAdam\n",
      "epoch: 6000, acc: 0.950, loss:0.110, lr: 0.018868102529269144, method: OptimizerAdam\n",
      "epoch: 7000, acc: 0.953, loss:0.098, lr: 0.018691763474424996, method: OptimizerAdam\n",
      "epoch: 8000, acc: 0.957, loss:0.094, lr: 0.01851868998787026, method: OptimizerAdam\n",
      "epoch: 9000, acc: 0.957, loss:0.088, lr: 0.018348792190754044, method: OptimizerAdam\n",
      "epoch: 10000, acc: 0.957, loss:0.085, lr: 0.018181983472577025, method: OptimizerAdam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for  optimizer in optimizer_class: \n",
    "    \n",
    "    for epoch in range (10001):\n",
    "        \n",
    "\n",
    "        dense1.forward(X)\n",
    "        #this is whhen input * weight + bias\n",
    "        # Perform a forward pass through activation function\n",
    "        # takes the output of first dense layer here\n",
    "\n",
    "        activation1.forward(dense1.output) #this is relu\n",
    "        # Perform a forward pass through second Dense layer\n",
    "        # takes outputs of activation function of first layer as inputs\n",
    "\n",
    "        dense2.forward(activation1.output) #this is input * weight + bias\n",
    "        # Perform a forward pass through the activation/loss function\n",
    "        # takes the output of second dense layer here and returns loss\n",
    "\n",
    "        loss = loss_activation.forward(dense2.output, y)\n",
    "        # Let's see output of the first few samples:\n",
    "\n",
    "        predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "        if len (y.shape) == 2 :\n",
    "            y = np.argmax(y, axis = 1 )\n",
    "        accuracy = np.mean(predictions == y)\n",
    "\n",
    "        if not epoch % 1000:\n",
    "        \n",
    "\n",
    "            print(f'epoch: {epoch}, ' + \n",
    "                    f'acc: {accuracy:.3f}, ' +\n",
    "                    f'loss:{loss:.3f}, ' +\n",
    "                    f'lr: {optimizer.current_learning_rate}, '+\n",
    "                    f'method: {optimizer.name}')\n",
    "\n",
    "            scores[\"method\"].append(optimizer.name)\n",
    "            scores[\"epoch\"].append(epoch)\n",
    "            scores[\"acc\"].append(accuracy)\n",
    "            scores[\"loss\"].append(loss)\n",
    "            scores[\"lr\"].append(optimizer.current_learning_rate)       \n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y)\n",
    "        dense2.backward(loss_activation.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "            \n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: 10000, acc: 0.867, loss:0.223, lr: 1.0, Using Basic SGD\n",
    "epoch: 10000, acc: 0.867, loss:0.224, lr: 0.85, Using Basic SGD with learning rate 0.85\n",
    "\n",
    "First try SGD with 0.85\n",
    "Then learning rate decay\n",
    "Then SGD with momentum\n",
    "Then adaptive gradient\n",
    "Then RMS Prop\n",
    "Then adaptive momentum\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24e68ee5c3be1453adda2c12808546301ced8adfec32f06d9e3ee889c2d5adb6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
