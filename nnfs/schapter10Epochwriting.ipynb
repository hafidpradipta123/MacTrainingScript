{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples =100, classes = 3)\n",
    "f'Xshape = {X.shape} and yshape = {y.shape}'\n",
    "\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD\n",
    "\n",
    "for epoch in range(11):\n",
    "    print(f'this is {epoch +1} iteration')\n",
    "    dense1.forward(X)\n",
    "    print(f'whats happening in dense1.forward(X) is we have X which has a dimension of {X.shape}')\n",
    "    print(f'we initiate weight {dense1.weights.shape} and bias {dense1.biases.shape}')\n",
    "    print(f'dot product of the input {X.shape} and weight {dense1.weights.shape}, makes it {dense1.output.shape} plus bias {dense1.biases.shape}')\n",
    "    print(f'we will have hte result of {dense1.output.shape}\\n')\n",
    "\n",
    "    activation1.forward(dense1.output)\n",
    "    print(f'this is the ReLU activation function. any value less than zero. we make it zero\\n')\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    print(f'whats happening in dense2.forward(activation1.output) is we have activation1.output which has a dimension of {activation1.output.shape}')\n",
    "    print(f'we initiate weight {dense2.weights.shape} and bias {dense2.biases.shape}')\n",
    "    print(f'dot product of the input {X.shape} and weight {dense2.weights.shape}, makes it {dense2.output.shape} plus bias {dense2.biases.shape}')\n",
    "    print(f'we will have the result of {dense2.output.shape}\\n')\n",
    "\n",
    "    print(f'loss = loss_activation.forward(dense2.output, y)')\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    print(f'in this loss function, we use the softmax method which is ')\n",
    "    print(f'we transform the inputs into -inf and 0 to avoid overflow')\n",
    "    print(f' then we normalize dense2.output so we got the probabilities. which is yhat')\n",
    "    print(f'dimension of yhat is {loss_activation.output.shape}')\n",
    "    print(f' Then we calculate the difference between loss_activation.output and y (aka y_true) ')\n",
    "    print(f' the calculation follow LossCategoricalCrossentropy. ')\n",
    "    print(f' we have the length of y_true is {len(y.shape)}') \n",
    "    print(f'which is dense layer not one hot encoding')\n",
    "    print(f' we got the correct confidences by taking each matrix from 0 to n, to its designtaed y_true. refer to the code\\n')\n",
    "\n",
    "    print(f'predictions = np.argmax(loss_activation.output, axis = 1 )')\n",
    "    print(f'predictions is the highest number of predictions in each row')\n",
    "    print(f' then we compare it with the true y using this formula if len (y.shape) == 2 :        y = np.argmax(y, axis = 1 )')\n",
    "    print(f' which is if it is one hot encoding, we want to make them layer dense.')\n",
    "    print(f' then we average the accuracy,we give points when predictions is equal to true_y')\n",
    "\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    print(f' acc:{accuracy}')\n",
    "\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    print('loss_activation.backward(loss_activation.output, y)')\n",
    "    print('remember that we are looking for partial derivative of loss categorical cross entropy with respect to the inputs.')\n",
    "    print('dvalues is loss_activaiton.output and partial derivative is yhat - y so that ')\n",
    "\n",
    "\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    print(f'end of iteration \\n \\n')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
