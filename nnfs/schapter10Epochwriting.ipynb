{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.357 , loss: 1.099 \n",
      "epoch: 100 , acc: 0.437 , loss: 1.074 \n",
      "epoch: 200 , acc: 0.430 , loss: 1.066 \n",
      "epoch: 300 , acc: 0.420 , loss: 1.065 \n",
      "epoch: 400 , acc: 0.413 , loss: 1.064 \n",
      "epoch: 500 , acc: 0.410 , loss: 1.064 \n",
      "epoch: 600 , acc: 0.410 , loss: 1.063 \n",
      "epoch: 700 , acc: 0.413 , loss: 1.062 \n",
      "epoch: 800 , acc: 0.417 , loss: 1.060 \n",
      "epoch: 900 , acc: 0.417 , loss: 1.056 \n",
      "epoch: 1000 , acc: 0.433 , loss: 1.049 \n",
      "epoch: 1100 , acc: 0.430 , loss: 1.038 \n",
      "epoch: 1200 , acc: 0.437 , loss: 1.026 \n",
      "epoch: 1300 , acc: 0.453 , loss: 1.022 \n",
      "epoch: 1400 , acc: 0.457 , loss: 1.021 \n",
      "epoch: 1500 , acc: 0.467 , loss: 1.030 \n",
      "epoch: 1600 , acc: 0.413 , loss: 1.022 \n",
      "epoch: 1700 , acc: 0.393 , loss: 1.029 \n",
      "epoch: 1800 , acc: 0.497 , loss: 1.022 \n",
      "epoch: 1900 , acc: 0.450 , loss: 1.009 \n",
      "epoch: 2000 , acc: 0.460 , loss: 1.011 \n",
      "epoch: 2100 , acc: 0.490 , loss: 1.031 \n",
      "epoch: 2200 , acc: 0.477 , loss: 1.025 \n",
      "epoch: 2300 , acc: 0.447 , loss: 1.012 \n",
      "epoch: 2400 , acc: 0.483 , loss: 1.007 \n",
      "epoch: 2500 , acc: 0.470 , loss: 1.001 \n",
      "epoch: 2600 , acc: 0.473 , loss: 0.997 \n",
      "epoch: 2700 , acc: 0.523 , loss: 0.997 \n",
      "epoch: 2800 , acc: 0.510 , loss: 0.987 \n",
      "epoch: 2900 , acc: 0.503 , loss: 0.971 \n",
      "epoch: 3000 , acc: 0.520 , loss: 0.986 \n",
      "epoch: 3100 , acc: 0.507 , loss: 0.965 \n",
      "epoch: 3200 , acc: 0.523 , loss: 0.966 \n",
      "epoch: 3300 , acc: 0.470 , loss: 0.980 \n",
      "epoch: 3400 , acc: 0.523 , loss: 0.951 \n",
      "epoch: 3500 , acc: 0.507 , loss: 0.983 \n",
      "epoch: 3600 , acc: 0.520 , loss: 0.954 \n",
      "epoch: 3700 , acc: 0.523 , loss: 0.968 \n",
      "epoch: 3800 , acc: 0.543 , loss: 0.944 \n",
      "epoch: 3900 , acc: 0.523 , loss: 0.942 \n",
      "epoch: 4000 , acc: 0.503 , loss: 0.936 \n",
      "epoch: 4100 , acc: 0.497 , loss: 0.990 \n",
      "epoch: 4200 , acc: 0.540 , loss: 0.936 \n",
      "epoch: 4300 , acc: 0.557 , loss: 0.921 \n",
      "epoch: 4400 , acc: 0.567 , loss: 0.901 \n",
      "epoch: 4500 , acc: 0.557 , loss: 0.920 \n",
      "epoch: 4600 , acc: 0.570 , loss: 0.906 \n",
      "epoch: 4700 , acc: 0.573 , loss: 0.920 \n",
      "epoch: 4800 , acc: 0.547 , loss: 0.911 \n",
      "epoch: 4900 , acc: 0.547 , loss: 0.954 \n",
      "epoch: 5000 , acc: 0.580 , loss: 0.912 \n",
      "epoch: 5100 , acc: 0.560 , loss: 0.923 \n",
      "epoch: 5200 , acc: 0.537 , loss: 0.904 \n",
      "epoch: 5300 , acc: 0.543 , loss: 0.941 \n",
      "epoch: 5400 , acc: 0.580 , loss: 0.889 \n",
      "epoch: 5500 , acc: 0.517 , loss: 0.936 \n",
      "epoch: 5600 , acc: 0.533 , loss: 0.899 \n",
      "epoch: 5700 , acc: 0.560 , loss: 0.909 \n",
      "epoch: 5800 , acc: 0.570 , loss: 0.892 \n",
      "epoch: 5900 , acc: 0.563 , loss: 0.879 \n",
      "epoch: 6000 , acc: 0.560 , loss: 0.892 \n",
      "epoch: 6100 , acc: 0.557 , loss: 0.890 \n",
      "epoch: 6200 , acc: 0.567 , loss: 0.876 \n",
      "epoch: 6300 , acc: 0.520 , loss: 0.907 \n",
      "epoch: 6400 , acc: 0.540 , loss: 0.877 \n",
      "epoch: 6500 , acc: 0.540 , loss: 0.914 \n",
      "epoch: 6600 , acc: 0.567 , loss: 0.898 \n",
      "epoch: 6700 , acc: 0.563 , loss: 0.889 \n",
      "epoch: 6800 , acc: 0.567 , loss: 0.884 \n",
      "epoch: 6900 , acc: 0.593 , loss: 0.866 \n",
      "epoch: 7000 , acc: 0.520 , loss: 0.917 \n",
      "epoch: 7100 , acc: 0.553 , loss: 0.872 \n",
      "epoch: 7200 , acc: 0.597 , loss: 0.883 \n",
      "epoch: 7300 , acc: 0.540 , loss: 0.909 \n",
      "epoch: 7400 , acc: 0.577 , loss: 0.876 \n",
      "epoch: 7500 , acc: 0.570 , loss: 0.875 \n",
      "epoch: 7600 , acc: 0.560 , loss: 0.866 \n",
      "epoch: 7700 , acc: 0.610 , loss: 0.857 \n",
      "epoch: 7800 , acc: 0.617 , loss: 0.856 \n",
      "epoch: 7900 , acc: 0.543 , loss: 0.886 \n",
      "epoch: 8000 , acc: 0.553 , loss: 0.883 \n",
      "epoch: 8100 , acc: 0.573 , loss: 0.854 \n",
      "epoch: 8200 , acc: 0.573 , loss: 0.853 \n",
      "epoch: 8300 , acc: 0.627 , loss: 0.845 \n",
      "epoch: 8400 , acc: 0.607 , loss: 0.854 \n",
      "epoch: 8500 , acc: 0.553 , loss: 0.900 \n",
      "epoch: 8600 , acc: 0.583 , loss: 0.860 \n",
      "epoch: 8700 , acc: 0.580 , loss: 0.858 \n",
      "epoch: 8800 , acc: 0.583 , loss: 0.858 \n",
      "epoch: 8900 , acc: 0.587 , loss: 0.855 \n",
      "epoch: 9000 , acc: 0.613 , loss: 0.854 \n",
      "epoch: 9100 , acc: 0.547 , loss: 0.873 \n",
      "epoch: 9200 , acc: 0.573 , loss: 0.847 \n",
      "epoch: 9300 , acc: 0.620 , loss: 0.845 \n",
      "epoch: 9400 , acc: 0.557 , loss: 0.864 \n",
      "epoch: 9500 , acc: 0.603 , loss: 0.846 \n",
      "epoch: 9600 , acc: 0.580 , loss: 0.845 \n",
      "epoch: 9700 , acc: 0.617 , loss: 0.847 \n",
      "epoch: 9800 , acc: 0.560 , loss: 0.858 \n",
      "epoch: 9900 , acc: 0.590 , loss: 0.833 \n",
      "epoch: 10000 , acc: 0.577 , loss: 0.849 \n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense( 2 , 64 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 64 , 3 )\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate =)\n",
    "for epoch in range ( 10001 ):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "        f'acc: {accuracy :.3f} , ' +\n",
    "        f'loss: {loss :.3f} ' )\n",
    "            # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats happening in dense1.forward(X) is we have X which has a dimension of (300, 2)\n",
      "we initiate weight (2, 64) and bias (1, 64)\n",
      "dot product of the input (300, 2) and weight (2, 64), makes it (300, 64) plus bias (1, 64)\n",
      "we will have hte result of (300, 64)\n",
      "\n",
      "activation1.forward(dense1.output)\n",
      "this is the ReLU activation function. any value less than zero. we make it zero\n",
      "\n",
      "whats happening in dense2.forward(activation1.output) is we have activation1.output which has a dimension of (300, 64)\n",
      "we initiate weight (64, 3) and bias (1, 3)\n",
      "dot product of the input (300, 2) and weight (64, 3), makes it (300, 3) plus bias (1, 3)\n",
      "we will have the result of (300, 3)\n",
      "\n",
      "loss = loss_activation.forward(dense2.output, y)\n",
      "in this loss function, we use the softmax method which is \n",
      "we transform the inputs into -inf and 0 to avoid overflow\n",
      " then we normalize dense2.output so we got the probabilities. which is yhat\n",
      "dimension of yhat is (300, 3)\n",
      " Then we calculate the difference between loss_activation.output and y (aka y_true) \n",
      " the calculation follow LossCategoricalCrossentropy. \n",
      " we have the length of y_true is 1\n",
      "which is dense layer not one hot encoding\n",
      " we got the correct confidences by taking each matrix from 0 to n, to its designtaed y_true. refer to the code\n",
      "\n",
      "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
      "predictions is the highest number of predictions in each row\n",
      " then we compare it with the true y using this formula if len (y.shape) == 2 :        y = np.argmax(y, axis = 1 )\n",
      " which is if it is one hot encoding, we want to make them layer dense.\n",
      " then we average the accuracy,we give points when predictions is equal to true_y\n",
      " acc:0.35\n",
      "loss_activation.backward(loss_activation.output, y)\n",
      "remember that we are looking for partial derivative of loss categorical cross entropy with respect to the inputs.\n",
      "dvalues is loss_activaiton.output and partial derivative is yhat - y so that \n",
      "\n",
      "dense2.backward(loss_activation.dinputs)\n",
      "we are going to have dweights, dbiases and dinputs. partial\n",
      "\n",
      "activation1.backward(dense2.dinputs)\n",
      "backwardRelu, if inpus, which is dense2.dinputs is less than 0, we make it zero\n",
      "dense1.backward(loss_activation.dinputs)\n",
      "we are going to have dweights, dbiases and dinputs. partial\n",
      "\n",
      "update weights and biases with -learning.rate * matrix of weights\n",
      "end of iteration \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(samples =100, classes = 3)\n",
    "f'Xshape = {X.shape} and yshape = {y.shape}'\n",
    "\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "dense1.forward(X)\n",
    "print(f'whats happening in dense1.forward(X) is we have X which has a dimension of {X.shape}')\n",
    "print(f'we initiate weight {dense1.weights.shape} and bias {dense1.biases.shape}')\n",
    "print(f'dot product of the input {X.shape} and weight {dense1.weights.shape}, makes it {dense1.output.shape} plus bias {dense1.biases.shape}')\n",
    "print(f'we will have hte result of {dense1.output.shape}\\n')\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "print('activation1.forward(dense1.output)')\n",
    "print(f'this is the ReLU activation function. any value less than zero. we make it zero\\n')\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "print(f'whats happening in dense2.forward(activation1.output) is we have activation1.output which has a dimension of {activation1.output.shape}')\n",
    "print(f'we initiate weight {dense2.weights.shape} and bias {dense2.biases.shape}')\n",
    "print(f'dot product of the input {X.shape} and weight {dense2.weights.shape}, makes it {dense2.output.shape} plus bias {dense2.biases.shape}')\n",
    "print(f'we will have the result of {dense2.output.shape}\\n')\n",
    "\n",
    "print(f'loss = loss_activation.forward(dense2.output, y)')\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "print(f'in this loss function, we use the softmax method which is ')\n",
    "print(f'we transform the inputs into -inf and 0 to avoid overflow')\n",
    "print(f' then we normalize dense2.output so we got the probabilities. which is yhat')\n",
    "print(f'dimension of yhat is {loss_activation.output.shape}')\n",
    "print(f' Then we calculate the difference between loss_activation.output and y (aka y_true) ')\n",
    "print(f' the calculation follow LossCategoricalCrossentropy. ')\n",
    "print(f' we have the length of y_true is {len(y.shape)}') \n",
    "print(f'which is dense layer not one hot encoding')\n",
    "print(f' we got the correct confidences by taking each matrix from 0 to n, to its designtaed y_true. refer to the code\\n')\n",
    "\n",
    "print(f'predictions = np.argmax(loss_activation.output, axis = 1 )')\n",
    "print(f'predictions is the highest number of predictions in each row')\n",
    "print(f' then we compare it with the true y using this formula if len (y.shape) == 2 :        y = np.argmax(y, axis = 1 )')\n",
    "print(f' which is if it is one hot encoding, we want to make them layer dense.')\n",
    "print(f' then we average the accuracy,we give points when predictions is equal to true_y')\n",
    "\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "if len (y.shape) == 2 :\n",
    "    y = np.argmax(y, axis = 1 )\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f' acc:{accuracy}')\n",
    "\n",
    "\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "print('loss_activation.backward(loss_activation.output, y)')\n",
    "print('remember that we are looking for partial derivative of loss categorical cross entropy with respect to the inputs.')\n",
    "print('dvalues is loss_activaiton.output and partial derivative is yhat - y so that \\n')\n",
    "\n",
    "\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "print('dense2.backward(loss_activation.dinputs)')\n",
    "print('we are going to have dweights, dbiases and dinputs. partial\\n') \n",
    "activation1.backward(dense2.dinputs)\n",
    "print('activation1.backward(dense2.dinputs)')\n",
    "print('backwardRelu, if inpus, which is dense2.dinputs is less than 0, we make it zero')\n",
    "dense1.backward(activation1.dinputs)\n",
    "print('dense1.backward(loss_activation.dinputs)')\n",
    "print('we are going to have dweights, dbiases and dinputs. partial\\n')\n",
    "# Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "print(\"update weights and biases with -learning.rate * matrix of weights\")\n",
    "optimizer.update_params(dense2)\n",
    "print(f'end of iteration \\n \\n')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
