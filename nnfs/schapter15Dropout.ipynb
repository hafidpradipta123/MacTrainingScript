{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import pandas as pd\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1 = 0,\n",
    "    bias_regularizer_l1 =0, weight_regularizer_l2 = 0, bias_regularizer_l2 = 0):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "\n",
    "        if self.weight_regularizer_l1>0:\n",
    "            dl1 = np.ones_like(self.weights)\n",
    "            dl1[self.weights<0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dl1\n",
    "        \n",
    "        if self.weight_regularizer_l2 > 0 :\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0 :\n",
    "            dl1 = np.ones_like(self.biases)\n",
    "            dl1[self.biases<0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dl1\n",
    "        \n",
    "        if self.bias_regularizer_l2 > 0 :\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(layer.abs(layer.biases))\n",
    "\n",
    "        if layer.bias_regularizer_l2 > 0 :\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation.output #the output is a probability\n",
    "        return self.loss.calculate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate = 1., decay  = 0., momentum  = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.name = f'OptimizerSGD lr:{self.learning_rate}, decay ={self.decay}, momentum = {self.momentum}'\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "        #if we use momentum\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases  \n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    def __init__(self, learning_rate = 1, decay  = 0.,  epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'Adagrad decay =  {self.decay}'\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases  / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self, learning_rate = 0.001, decay  = 0., rho = 0.9, epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'RMSProp =  {self.decay}'\n",
    "        self.rho = rho\n",
    "        \n",
    "\n",
    "    def pre_udpate_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + ( 1- self.rho) * layer.dweights **2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + ( 1- self.rho) * layer.dbiases **2\n",
    "\n",
    "\n",
    "        layer.weights += -self.learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.learning_rate * layer.dbiases  / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate = 0.001, decay  = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.current_decay = decay\n",
    "        self.epsilon = epsilon #is only to prevent division by 0\n",
    "        self.name = f'OptimizerAdam lr =  {self.learning_rate}, decay = {self.current_decay}'\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "        \n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_decay = 1./ (1.+ self.decay * self.iterations)\n",
    "            self.current_learning_rate = self.learning_rate * (1./ (1.+ self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        #if we use momentum\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1-self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1- self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1- self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1- self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1- self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        #get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1- self.beta_2 ** (self.iterations +1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1- self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "            \n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Layer_Dropout:\n",
    "    def __init__(self, droprate):\n",
    "        self.rate = 1- droprate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random_binomial(1, self.rate, size=inputs.shape / self.rate)\n",
    "        #1 is onnly zero and one, self.rate is the probability of success. which is (1- rate )\n",
    "        self.output = inputs*self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , acc: 0.333 , loss: 1.099 (data_loss: 1.099 , reg_loss: 0.000 ), lr: 0.02 \n",
      "epoch: 100 , acc: 0.742 , loss: 0.730 (data_loss: 0.671 , reg_loss: 0.059 ), lr: 0.019999010049002574 \n",
      "epoch: 200 , acc: 0.820 , loss: 0.572 (data_loss: 0.483 , reg_loss: 0.089 ), lr: 0.019998010197985302 \n",
      "epoch: 300 , acc: 0.859 , loss: 0.503 (data_loss: 0.406 , reg_loss: 0.097 ), lr: 0.019997010446938183 \n",
      "epoch: 400 , acc: 0.875 , loss: 0.463 (data_loss: 0.366 , reg_loss: 0.097 ), lr: 0.01999601079584623 \n",
      "epoch: 500 , acc: 0.884 , loss: 0.436 (data_loss: 0.341 , reg_loss: 0.095 ), lr: 0.01999501124469445 \n",
      "epoch: 600 , acc: 0.874 , loss: 0.422 (data_loss: 0.330 , reg_loss: 0.092 ), lr: 0.01999401179346786 \n",
      "epoch: 700 , acc: 0.887 , loss: 0.405 (data_loss: 0.314 , reg_loss: 0.092 ), lr: 0.01999301244215147 \n",
      "epoch: 800 , acc: 0.890 , loss: 0.393 (data_loss: 0.305 , reg_loss: 0.088 ), lr: 0.0199920131907303 \n",
      "epoch: 900 , acc: 0.887 , loss: 0.387 (data_loss: 0.302 , reg_loss: 0.085 ), lr: 0.019991014039189386 \n",
      "epoch: 1000 , acc: 0.893 , loss: 0.375 (data_loss: 0.292 , reg_loss: 0.082 ), lr: 0.019990014987513734 \n",
      "epoch: 1100 , acc: 0.892 , loss: 0.368 (data_loss: 0.288 , reg_loss: 0.080 ), lr: 0.01998901603568839 \n",
      "epoch: 1200 , acc: 0.893 , loss: 0.362 (data_loss: 0.284 , reg_loss: 0.078 ), lr: 0.019988017183698373 \n",
      "epoch: 1300 , acc: 0.894 , loss: 0.356 (data_loss: 0.280 , reg_loss: 0.076 ), lr: 0.01998701843152872 \n",
      "epoch: 1400 , acc: 0.896 , loss: 0.354 (data_loss: 0.280 , reg_loss: 0.074 ), lr: 0.019986019779164473 \n",
      "epoch: 1500 , acc: 0.899 , loss: 0.345 (data_loss: 0.273 , reg_loss: 0.072 ), lr: 0.019985021226590672 \n",
      "epoch: 1600 , acc: 0.886 , loss: 0.349 (data_loss: 0.278 , reg_loss: 0.071 ), lr: 0.01998402277379235 \n",
      "epoch: 1700 , acc: 0.893 , loss: 0.344 (data_loss: 0.275 , reg_loss: 0.069 ), lr: 0.01998302442075457 \n",
      "epoch: 1800 , acc: 0.899 , loss: 0.332 (data_loss: 0.264 , reg_loss: 0.068 ), lr: 0.019982026167462367 \n",
      "epoch: 1900 , acc: 0.900 , loss: 0.337 (data_loss: 0.270 , reg_loss: 0.067 ), lr: 0.019981028013900805 \n",
      "epoch: 2000 , acc: 0.896 , loss: 0.368 (data_loss: 0.283 , reg_loss: 0.085 ), lr: 0.019980029960054924 \n",
      "epoch: 2100 , acc: 0.898 , loss: 0.355 (data_loss: 0.274 , reg_loss: 0.080 ), lr: 0.019979032005909798 \n",
      "epoch: 2200 , acc: 0.899 , loss: 0.348 (data_loss: 0.270 , reg_loss: 0.078 ), lr: 0.01997803415145048 \n",
      "epoch: 2300 , acc: 0.899 , loss: 0.343 (data_loss: 0.267 , reg_loss: 0.076 ), lr: 0.019977036396662037 \n",
      "epoch: 2400 , acc: 0.900 , loss: 0.339 (data_loss: 0.265 , reg_loss: 0.074 ), lr: 0.019976038741529537 \n",
      "epoch: 2500 , acc: 0.900 , loss: 0.335 (data_loss: 0.263 , reg_loss: 0.072 ), lr: 0.01997504118603805 \n",
      "epoch: 2600 , acc: 0.901 , loss: 0.332 (data_loss: 0.262 , reg_loss: 0.071 ), lr: 0.01997404373017264 \n",
      "epoch: 2700 , acc: 0.902 , loss: 0.329 (data_loss: 0.260 , reg_loss: 0.069 ), lr: 0.0199730463739184 \n",
      "epoch: 2800 , acc: 0.899 , loss: 0.328 (data_loss: 0.261 , reg_loss: 0.068 ), lr: 0.019972049117260395 \n",
      "epoch: 2900 , acc: 0.899 , loss: 0.326 (data_loss: 0.260 , reg_loss: 0.066 ), lr: 0.019971051960183714 \n",
      "epoch: 3000 , acc: 0.900 , loss: 0.323 (data_loss: 0.257 , reg_loss: 0.065 ), lr: 0.019970054902673444 \n",
      "epoch: 3100 , acc: 0.904 , loss: 0.318 (data_loss: 0.254 , reg_loss: 0.064 ), lr: 0.019969057944714663 \n",
      "epoch: 3200 , acc: 0.904 , loss: 0.316 (data_loss: 0.253 , reg_loss: 0.063 ), lr: 0.019968061086292475 \n",
      "epoch: 3300 , acc: 0.901 , loss: 0.319 (data_loss: 0.257 , reg_loss: 0.062 ), lr: 0.019967064327391967 \n",
      "epoch: 3400 , acc: 0.901 , loss: 0.317 (data_loss: 0.256 , reg_loss: 0.061 ), lr: 0.019966067667998237 \n",
      "epoch: 3500 , acc: 0.905 , loss: 0.311 (data_loss: 0.251 , reg_loss: 0.060 ), lr: 0.019965071108096383 \n",
      "epoch: 3600 , acc: 0.907 , loss: 0.306 (data_loss: 0.247 , reg_loss: 0.059 ), lr: 0.01996407464767152 \n",
      "epoch: 3700 , acc: 0.905 , loss: 0.306 (data_loss: 0.248 , reg_loss: 0.059 ), lr: 0.019963078286708732 \n",
      "epoch: 3800 , acc: 0.903 , loss: 0.307 (data_loss: 0.249 , reg_loss: 0.058 ), lr: 0.019962082025193145 \n",
      "epoch: 3900 , acc: 0.903 , loss: 0.306 (data_loss: 0.249 , reg_loss: 0.057 ), lr: 0.019961085863109868 \n",
      "epoch: 4000 , acc: 0.908 , loss: 0.299 (data_loss: 0.243 , reg_loss: 0.056 ), lr: 0.019960089800444013 \n",
      "epoch: 4100 , acc: 0.908 , loss: 0.297 (data_loss: 0.242 , reg_loss: 0.056 ), lr: 0.019959093837180697 \n",
      "epoch: 4200 , acc: 0.899 , loss: 0.311 (data_loss: 0.256 , reg_loss: 0.055 ), lr: 0.01995809797330505 \n",
      "epoch: 4300 , acc: 0.905 , loss: 0.297 (data_loss: 0.243 , reg_loss: 0.054 ), lr: 0.01995710220880218 \n",
      "epoch: 4400 , acc: 0.905 , loss: 0.296 (data_loss: 0.242 , reg_loss: 0.054 ), lr: 0.019956106543657228 \n",
      "epoch: 4500 , acc: 0.908 , loss: 0.293 (data_loss: 0.240 , reg_loss: 0.053 ), lr: 0.019955110977855316 \n",
      "epoch: 4600 , acc: 0.906 , loss: 0.293 (data_loss: 0.240 , reg_loss: 0.052 ), lr: 0.01995411551138158 \n",
      "epoch: 4700 , acc: 0.907 , loss: 0.289 (data_loss: 0.237 , reg_loss: 0.052 ), lr: 0.019953120144221154 \n",
      "epoch: 4800 , acc: 0.902 , loss: 0.299 (data_loss: 0.247 , reg_loss: 0.051 ), lr: 0.019952124876359174 \n",
      "epoch: 4900 , acc: 0.902 , loss: 0.319 (data_loss: 0.253 , reg_loss: 0.066 ), lr: 0.01995112970778079 \n",
      "epoch: 5000 , acc: 0.905 , loss: 0.310 (data_loss: 0.246 , reg_loss: 0.063 ), lr: 0.019950134638471142 \n",
      "epoch: 5100 , acc: 0.905 , loss: 0.306 (data_loss: 0.244 , reg_loss: 0.062 ), lr: 0.019949139668415376 \n",
      "epoch: 5200 , acc: 0.905 , loss: 0.303 (data_loss: 0.243 , reg_loss: 0.061 ), lr: 0.01994814479759864 \n",
      "epoch: 5300 , acc: 0.905 , loss: 0.301 (data_loss: 0.242 , reg_loss: 0.060 ), lr: 0.019947150026006097 \n",
      "epoch: 5400 , acc: 0.906 , loss: 0.299 (data_loss: 0.240 , reg_loss: 0.059 ), lr: 0.019946155353622895 \n",
      "epoch: 5500 , acc: 0.906 , loss: 0.297 (data_loss: 0.239 , reg_loss: 0.058 ), lr: 0.019945160780434196 \n",
      "epoch: 5600 , acc: 0.907 , loss: 0.295 (data_loss: 0.238 , reg_loss: 0.057 ), lr: 0.019944166306425162 \n",
      "epoch: 5700 , acc: 0.908 , loss: 0.293 (data_loss: 0.237 , reg_loss: 0.056 ), lr: 0.01994317193158096 \n",
      "epoch: 5800 , acc: 0.907 , loss: 0.293 (data_loss: 0.238 , reg_loss: 0.055 ), lr: 0.019942177655886757 \n",
      "epoch: 5900 , acc: 0.909 , loss: 0.290 (data_loss: 0.236 , reg_loss: 0.054 ), lr: 0.019941183479327725 \n",
      "epoch: 6000 , acc: 0.905 , loss: 0.293 (data_loss: 0.239 , reg_loss: 0.054 ), lr: 0.019940189401889033 \n",
      "epoch: 6100 , acc: 0.910 , loss: 0.287 (data_loss: 0.234 , reg_loss: 0.053 ), lr: 0.01993919542355587 \n",
      "epoch: 6200 , acc: 0.907 , loss: 0.290 (data_loss: 0.237 , reg_loss: 0.052 ), lr: 0.019938201544313403 \n",
      "epoch: 6300 , acc: 0.906 , loss: 0.294 (data_loss: 0.242 , reg_loss: 0.051 ), lr: 0.01993720776414682 \n",
      "epoch: 6400 , acc: 0.908 , loss: 0.287 (data_loss: 0.236 , reg_loss: 0.051 ), lr: 0.019936214083041307 \n",
      "epoch: 6500 , acc: 0.909 , loss: 0.286 (data_loss: 0.235 , reg_loss: 0.050 ), lr: 0.01993522050098206 \n",
      "epoch: 6600 , acc: 0.912 , loss: 0.281 (data_loss: 0.231 , reg_loss: 0.050 ), lr: 0.019934227017954262 \n",
      "epoch: 6700 , acc: 0.912 , loss: 0.280 (data_loss: 0.231 , reg_loss: 0.049 ), lr: 0.01993323363394311 \n",
      "epoch: 6800 , acc: 0.908 , loss: 0.281 (data_loss: 0.233 , reg_loss: 0.049 ), lr: 0.0199322403489338 \n",
      "epoch: 6900 , acc: 0.906 , loss: 0.284 (data_loss: 0.236 , reg_loss: 0.048 ), lr: 0.019931247162911534 \n",
      "epoch: 7000 , acc: 0.911 , loss: 0.277 (data_loss: 0.229 , reg_loss: 0.048 ), lr: 0.019930254075861523 \n",
      "epoch: 7100 , acc: 0.909 , loss: 0.279 (data_loss: 0.232 , reg_loss: 0.047 ), lr: 0.019929261087768962 \n",
      "epoch: 7200 , acc: 0.903 , loss: 0.283 (data_loss: 0.236 , reg_loss: 0.047 ), lr: 0.01992826819861907 \n",
      "epoch: 7300 , acc: 0.904 , loss: 0.283 (data_loss: 0.236 , reg_loss: 0.047 ), lr: 0.019927275408397054 \n",
      "epoch: 7400 , acc: 0.904 , loss: 0.281 (data_loss: 0.235 , reg_loss: 0.046 ), lr: 0.019926282717088132 \n",
      "epoch: 7500 , acc: 0.902 , loss: 0.292 (data_loss: 0.247 , reg_loss: 0.046 ), lr: 0.01992529012467752 \n",
      "epoch: 7600 , acc: 0.909 , loss: 0.296 (data_loss: 0.239 , reg_loss: 0.057 ), lr: 0.019924297631150445 \n",
      "epoch: 7700 , acc: 0.910 , loss: 0.289 (data_loss: 0.234 , reg_loss: 0.055 ), lr: 0.019923305236492123 \n",
      "epoch: 7800 , acc: 0.912 , loss: 0.286 (data_loss: 0.232 , reg_loss: 0.054 ), lr: 0.01992231294068779 \n",
      "epoch: 7900 , acc: 0.913 , loss: 0.284 (data_loss: 0.231 , reg_loss: 0.053 ), lr: 0.019921320743722666 \n",
      "epoch: 8000 , acc: 0.911 , loss: 0.283 (data_loss: 0.231 , reg_loss: 0.052 ), lr: 0.019920328645582 \n",
      "epoch: 8100 , acc: 0.909 , loss: 0.282 (data_loss: 0.231 , reg_loss: 0.051 ), lr: 0.019919336646251007 \n",
      "epoch: 8200 , acc: 0.912 , loss: 0.280 (data_loss: 0.230 , reg_loss: 0.051 ), lr: 0.019918344745714942 \n",
      "epoch: 8300 , acc: 0.911 , loss: 0.279 (data_loss: 0.229 , reg_loss: 0.050 ), lr: 0.019917352943959042 \n",
      "epoch: 8400 , acc: 0.904 , loss: 0.289 (data_loss: 0.240 , reg_loss: 0.049 ), lr: 0.019916361240968555 \n",
      "epoch: 8500 , acc: 0.911 , loss: 0.277 (data_loss: 0.228 , reg_loss: 0.049 ), lr: 0.01991536963672872 \n",
      "epoch: 8600 , acc: 0.912 , loss: 0.276 (data_loss: 0.228 , reg_loss: 0.048 ), lr: 0.019914378131224802 \n",
      "epoch: 8700 , acc: 0.908 , loss: 0.279 (data_loss: 0.231 , reg_loss: 0.047 ), lr: 0.01991338672444204 \n",
      "epoch: 8800 , acc: 0.911 , loss: 0.275 (data_loss: 0.228 , reg_loss: 0.047 ), lr: 0.0199123954163657 \n",
      "epoch: 8900 , acc: 0.906 , loss: 0.279 (data_loss: 0.233 , reg_loss: 0.046 ), lr: 0.019911404206981037 \n",
      "epoch: 9000 , acc: 0.907 , loss: 0.276 (data_loss: 0.230 , reg_loss: 0.046 ), lr: 0.019910413096273318 \n",
      "epoch: 9100 , acc: 0.912 , loss: 0.274 (data_loss: 0.228 , reg_loss: 0.045 ), lr: 0.019909422084227805 \n",
      "epoch: 9200 , acc: 0.904 , loss: 0.285 (data_loss: 0.240 , reg_loss: 0.045 ), lr: 0.019908431170829768 \n",
      "epoch: 9300 , acc: 0.905 , loss: 0.278 (data_loss: 0.234 , reg_loss: 0.045 ), lr: 0.01990744035606448 \n",
      "epoch: 9400 , acc: 0.907 , loss: 0.272 (data_loss: 0.228 , reg_loss: 0.044 ), lr: 0.01990644963991721 \n",
      "epoch: 9500 , acc: 0.865 , loss: 0.399 (data_loss: 0.338 , reg_loss: 0.061 ), lr: 0.01990545902237324 \n",
      "epoch: 9600 , acc: 0.909 , loss: 0.287 (data_loss: 0.235 , reg_loss: 0.052 ), lr: 0.019904468503417844 \n",
      "epoch: 9700 , acc: 0.909 , loss: 0.284 (data_loss: 0.233 , reg_loss: 0.051 ), lr: 0.019903478083036316 \n",
      "epoch: 9800 , acc: 0.909 , loss: 0.282 (data_loss: 0.233 , reg_loss: 0.050 ), lr: 0.019902487761213932 \n",
      "epoch: 9900 , acc: 0.910 , loss: 0.280 (data_loss: 0.231 , reg_loss: 0.049 ), lr: 0.019901497537935988 \n",
      "epoch: 10000 , acc: 0.908 , loss: 0.279 (data_loss: 0.230 , reg_loss: 0.049 ), lr: 0.019900507413187767 \n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples = 1000, classes = 3)\n",
    "\n",
    "dense1 = Layer_Dense(2,512, weight_regularizer_l2= 5e-4, bias_regularizer_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(512,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay = 5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    data_loss= loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1)+ loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "    if len (y.shape) == 2 :\n",
    "        y = np.argmax(y, axis = 1 )\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if not epoch % 100 :\n",
    "        print (f'epoch: {epoch} , ' +\n",
    "        f'acc: {accuracy :.3f} , ' +\n",
    "        f'loss: {loss :.3f} (' +\n",
    "        f'data_loss: {data_loss :.3f} , ' +\n",
    "        f'reg_loss: {regularization_loss :.3f} ), ' +\n",
    "        f'lr: {optimizer.current_learning_rate} ' )\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.897 , loss: 0.268\n"
     ]
    }
   ],
   "source": [
    "# Create test dataset\n",
    "X_test, y_test = spiral_data( samples = 100 , classes = 3 )\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis = 1 )\n",
    "if len (y_test.shape) == 2 :\n",
    "    y_test = np.argmax(y_test, axis = 1 )\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print (f'validation, acc: {accuracy :.3f} , loss: {loss :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum initial 0.36000000000000015 \n",
      "mean sum: 0.3654167500000001\n",
      "mean sum without normalizing: 0.3257465750000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dropout_rate = 0.2\n",
    "example_output = np.array([ 0.27 , - 1.03 , 0.67 , 0.99 , 0.05 ,\n",
    "- 0.37 , - 2.01 , 1.13 , - 0.07 , 0.73 ])\n",
    "print (f'sum initial { sum (example_output)} ' )\n",
    "sums = []\n",
    "for i in range ( 100000 ):\n",
    "    example_output2 = example_output * \\\n",
    "    np.random.binomial( 1 , 1 - dropout_rate, example_output.shape) / \\\n",
    "    ( 1 - dropout_rate)\n",
    "    sums.append( sum (example_output2))\n",
    "print (f'mean sum: {np.mean(sums)}')\n",
    "\n",
    "for i in range ( 100000 ):\n",
    "    example_output2 = example_output * \\\n",
    "    np.random.binomial( 1 , 1 - dropout_rate, example_output.shape)\n",
    "    sums.append( sum (example_output2))\n",
    "print (f'mean sum without normalizing: {np.mean(sums)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
