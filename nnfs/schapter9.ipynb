{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #self.inputs is the partial derivative of relu with respect to the input. meaning that if input < 0, there is no derivative or equal to 0\n",
    "    \n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues) #np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "\n",
    "        labels = len(dvalues[0]) #array([1,2,3])\n",
    "\n",
    "        #if y_true is [0,1,1]\n",
    "        #then np.eye will make it \n",
    "        #array([[1., 0., 0.],\n",
    "        #       [0., 1., 0.],\n",
    "        #       [0., 1., 0.]], dtype=float32)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true] \n",
    "    \n",
    "        self.dinputs = -y_true / dvalues #partial derivatives with respect tp inputs = matrix 3x3 - 3x3\n",
    "        #the derivative of this loss fucntion with respect ot is input = ground truth vector / vector of predicted values\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        #normalize to make the sum magnitude invariant to the number of samples. \n",
    "\n",
    "        \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs) #use softmax activation\n",
    "        self.output = self.activation_output #the output is a probability\n",
    "        return self.loss.caclulate(self.output, y_true) #calculate loss between predicted (self.output) and y_true\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1) #convert from one hot encoder to the discrete true labels\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs [range(samples) ,y_true] -= 1 #only at the given ytue, the value is minus by one. why?\n",
    "        #becayse the partial derivative of loss wrt of softmax function inputs. \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main codes\n",
    "# \n",
    "# import numpy as np\n",
    "import nnfs\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 =softmax_loss.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs # di assign daria tas\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "#make the matrix of the d values and then -class_targets / softmax_outputs then normalize using samples \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalues1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47619048 -0.         -0.        ]\n",
      " [-0.         -0.66666667 -0.        ]\n",
      " [-0.         -0.37037037 -0.        ]]\n",
      "[[-0.47619048 -0.         -0.        ]\n",
      " [-0.         -0.66666667 -0.        ]\n",
      " [-0.         -0.37037037 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "var1= np.eye(len(softmax_outputs[0]))[class_targets]\n",
    "var2 = -var1 / softmax_outputs\n",
    "var3 = var2/ len(softmax_outputs)\n",
    "print (var3)\n",
    "print(loss.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3   0.1   0.2 ]\n",
      " [ 0.1  -0.5   0.4 ]\n",
      " [ 0.02 -0.1   0.08]]\n"
     ]
    }
   ],
   "source": [
    "#test the code\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "#what happen in here\n",
    "samples = len(softmax_outputs)\n",
    "if len(class_targets.shape) == 2:#conver from one hot encoded to discrete true label\n",
    "            y_true = np.argmax(class_targets, axis = 1)\n",
    "\n",
    "\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "print(dvalues1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Activation_Softmax_Loss_CategoricalCrossentropy' object has no attribute 'activation_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_21636/611682820.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_21636/1664221105.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, y_true)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use softmax activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_output\u001b[0m \u001b[0;31m#the output is a probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaclulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#calculate loss between predicted (self.output) and y_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Activation_Softmax_Loss_CategoricalCrossentropy' object has no attribute 'activation_output'"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples = 69, classes = 3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "print(loss_activation.output[:5])\n",
    "print('loss:', loss)\n",
    "\n",
    "predictions = np.argamax(loss_activation.output, axis =1 )\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis = 1) #convert from one hot encoder to layer dense\n",
    "accuracy= np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss_CategoricalCrossentropy(Loss): explanation\n",
    "softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "[ 0 , 1 , 0 ],\n",
    "[ 0 , 1 , 0 ]])\n",
    "\n",
    "print(class_targets1)\n",
    "print(class_targets1.shape)\n",
    "print(len(class_targets1.shape))\n",
    "print(range ( len (softmax_outputs1)))\n",
    "correct_confidences1 = softmax_outputs1[range ( len (softmax_outputs1)),class_targets1]\n",
    "print(correct_confidences1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "softmax_output = np.array([[ 1,2,3,4 ], [5,6,7,8],    [9,10,11,12 ]] )\n",
    "dvalues = np.array([[ 1 , 2 , 3 ],[ 5 , 6 , 7 ],[ 9 , 10 , 11  ]])\n",
    "dinputs = np.empty_like(dvalues)\n",
    "class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "class_targets2 = np.array([[ 1 , 0 , 0 ],            [ 0 , 1 , 0 ],            [ 0 , 1 , 0 ]])\n",
    "#softmax_output = np.array(softmax_output).reshape( - 1 , 1 )\n",
    "np.eye(3)[class_targets1]\n",
    "dvc = dvalues.copy()\n",
    "dvc[range(3), class_targets1] -= 1\n",
    "print(dvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_output = softmax_output[0]\n",
    "single_output = np.array(single_output).reshape( - 1 , 1 )\n",
    "squared = np.dot(single_output, single_output.T)\n",
    "diagflat = np.diagflat(single_output)\n",
    "jacobian_matrix = np.diagflat(single_output) - squared\n",
    "firstdinputs =np.dot(jacobian_matrix,dvalues[0])\n",
    "print(single_output.shape)\n",
    "print(diagflat)\n",
    "print(single_output.T)\n",
    "print(single_output)\n",
    "print(squared)\n",
    "print(jacobian_matrix)\n",
    "print(dvalues[0])\n",
    "print(firstdinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dari buku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,(single_output, single_dvalues) in enumerate(zip(softmax_output, dvalues)):\n",
    "    single_output = single_output.reshape(-1,1)\n",
    "    jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "    dinputs[index] =np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "[ 0 , 1 , 0 ],\n",
    "[ 0 , 1 , 0 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_targets1)\n",
    "print(class_targets1.shape)\n",
    "print(len(class_targets1.shape))\n",
    "print(range ( len (softmax_outputs)))\n",
    "correct_confidences1 = softmax_outputs[range ( len (softmax_outputs)),class_targets1]\n",
    "print(correct_confidences1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_targets2)\n",
    "print(class_targets2.shape)\n",
    "print (len(class_targets2.shape))\n",
    "correct_confidences2 = np.sum(softmax_outputs * class_targets2,axis = 1)\n",
    "print(correct_confidences2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
