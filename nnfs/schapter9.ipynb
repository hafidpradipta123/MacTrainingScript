{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    #layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #derivative wrt of weights is inputs. Dimension adjustment is needed\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        #derivative of bias is column sums\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        #derivative wrt of inputs is weights\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs) #if the inputs is lower than 0, we make it 0, uf not, then we pass on\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy() #copy the gradient matrix\n",
    "        self.dinputs[self.inputs<= 0] = 0 #if it is lower than 0 then we'll make it zero\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #make the max = 1 and the min is -inf\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)# normalize\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues)  in enumerate(zip(self.output, dvalues)):\n",
    "            #single output is self.output[0]. it has the dimension 1xn, then the next line, we'll shift if to n,1\n",
    "            single_output = single_output.reshape(-1,1) #reshape the output to become (,1). n row with 1 column\n",
    "            \n",
    "            #diagflat is to craete the matrix where diagonal is a value and the rest is 0. then based on the formula \n",
    "            #diagflat will have n,n matrix with diagonal is single_output and the rest is 0\n",
    "            #then we have single.output where dim = n,1 and single.output.T dim = 1,n. this will result n,n\n",
    "            jacobian_matrix  = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            #jacobian matrix is n,n and single_values is n,1. Why Dvalues has n,n dimension and single is only the first row\n",
    "            #this will result n,1 dimension\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output,y) #output in here is the preddiction\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to avoid division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1 : \n",
    "            #softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "            #for example [0,1,1] just go to observation where it is y true. \n",
    "            #y_pred has a dimension nxn so for the first row, take index 0 y_pred_clipped[0,0]\n",
    "            #y_pred_clipped[1,1]\n",
    "            #y_pred_clipped[2,1]\n",
    "\n",
    "            correct_cofidences = y_pred_clipped[ range(samples), y_true]   \n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            #softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "            #[ 0.1 , 0.5 , 0.4 ],\n",
    "            #[ 0.02 , 0.9 , 0.08 ]])\n",
    "            #class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "            #[ 0 , 1 , 0 ],\n",
    "            #[ 0 , 1 , 0 ]])\n",
    "            #since this is one hot encoding. only 1 value is 1 and the rest is zero. so when multiplying, only the given 1 will yield a result\n",
    "            correct_confidences1 = np.sum(y_pred_clipped * y_true, axis = 1) \n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_cofidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss_CategoricalCrossentropy(Loss): explanation\n",
    "softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "[ 0 , 1 , 0 ],\n",
    "[ 0 , 1 , 0 ]])\n",
    "\n",
    "print(class_targets1)\n",
    "print(class_targets1.shape)\n",
    "print(len(class_targets1.shape))\n",
    "print(range ( len (softmax_outputs1)))\n",
    "correct_confidences1 = softmax_outputs1[range ( len (softmax_outputs1)),class_targets1]\n",
    "print(correct_confidences1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "softmax_output = np.array([[ 1,2,3,4 ], [5,6,7,8],    [9,10,11,12 ]] )\n",
    "dvalues = np.array([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]])\n",
    "dinputs = np.empty_like(dvalues)\n",
    "#softmax_output = np.array(softmax_output).reshape( - 1 , 1 )\n",
    "print(softmax_output)\n",
    "print(dvalues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n",
      "[[1 2 3 4]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]\n",
      " [ 4  8 12 16]]\n",
      "[[  0  -2  -3  -4]\n",
      " [ -2  -2  -6  -8]\n",
      " [ -3  -6  -6 -12]\n",
      " [ -4  -8 -12 -12]]\n",
      "[1 2 3 4]\n",
      "[ -29  -56  -81 -104]\n"
     ]
    }
   ],
   "source": [
    "single_output = softmax_output[0]\n",
    "single_output = np.array(single_output).reshape( - 1 , 1 )\n",
    "squared = np.dot(single_output, single_output.T)\n",
    "diagflat = np.diagflat(single_output)\n",
    "jacobian_matrix = np.diagflat(single_output) - squared\n",
    "firstdinputs =np.dot(jacobian_matrix,dvalues[0])\n",
    "print(single_output.shape)\n",
    "print(diagflat)\n",
    "print(single_output.T)\n",
    "print(single_output)\n",
    "print(squared)\n",
    "print(jacobian_matrix)\n",
    "print(dvalues[0])\n",
    "print(firstdinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dari buku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,(single_output, single_dvalues) in enumerate(zip(softmax_output, dvalues)):\n",
    "    single_output = single_output.reshape(-1,1)\n",
    "    jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "    dinputs[index] =np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Activation_Softmax_Loss_CategoricalCrossentropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zq/57t6_j4s4m3gptc9nylrv35m0000gn/T/ipykernel_11450/3216683093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclass_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msoftmax_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation_Softmax_Loss_CategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msoftmax_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdvalues1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msoftmax_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Activation_Softmax_Loss_CategoricalCrossentropy' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "\n",
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 =softmax_loss.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs1 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets1 = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "softmax_outputs2 = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "[ 0.1 , 0.5 , 0.4 ],\n",
    "[ 0.02 , 0.9 , 0.08 ]])\n",
    "class_targets2 = np.array([[ 1 , 0 , 0 ],\n",
    "[ 0 , 1 , 0 ],\n",
    "[ 0 , 1 , 0 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n",
      "(3,)\n",
      "1\n",
      "range(0, 3)\n",
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(class_targets1)\n",
    "print(class_targets1.shape)\n",
    "print(len(class_targets1.shape))\n",
    "print(range ( len (softmax_outputs)))\n",
    "correct_confidences1 = softmax_outputs[range ( len (softmax_outputs)),class_targets1]\n",
    "print(correct_confidences1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "(3, 3)\n",
      "2\n",
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(class_targets2)\n",
    "print(class_targets2.shape)\n",
    "print (len(class_targets2.shape))\n",
    "correct_confidences2 = np.sum(softmax_outputs * class_targets2,axis = 1)\n",
    "print(correct_confidences2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d2a999898ad509a5045ed7dea1feb70ce46febbdc78e231f5d65242874db573b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
